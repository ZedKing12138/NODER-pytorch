{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from Utils.Utls import *\n",
    "from Utils.Loss import *\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from Network.DynamicNet import DynamicNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#固定随机种子，让结果可以重复(后续若要进行uncertainty计算，可将此部分去掉)\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed) #CPU随机种子确定\n",
    "torch.cuda.manual_seed(seed) #GPU随机种子确定\n",
    "torch.cuda.manual_seed_all(seed) #所有的GPU设置种子\n",
    "torch.backends.cudnn.benchmark = False #模型卷积层预先优化关闭\n",
    "torch.backends.cudnn.deterministic = True #确定为默认卷积算法\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "data_root_path = \"./Mri_data/\"\n",
    "subject_path = \"002_S_4654\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#时间换算，将年月日格式统一变成天数\n",
    "def cal_time(visit_time):\n",
    "    separates = visit_time.split(\"-\")\n",
    "    year = int(separates[0])\n",
    "    flag_month = separates[1].split(\"0\")\n",
    "    month = int(separates[1]) if flag_month[0] != \"0\" else int(flag_month[1])\n",
    "    flag_day = separates[2].split(\"0\")\n",
    "    day = int(separates[2]) if flag_day[0] != \"0\" else int(flag_day[1])\n",
    "\n",
    "    return year*365+month*30+day\n",
    "\n",
    "\n",
    "#加载特定subject的图像和时间List\n",
    "def load_imgs_and_time(subject):\n",
    "    time_List = os.listdir(os.path.join(data_root_path,subject))\n",
    "    time_List = sorted(time_List,key = lambda x:cal_time(x))\n",
    "    img_list = []\n",
    "    for t in time_List:\n",
    "        img_list.append(load_nii(imgPath=os.path.join(data_root_path,subject,t,\"t1.nii.gz\")))\n",
    "    \n",
    "    return img_list,time_List\n",
    "\n",
    "#返回 图像的list，时间的list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(144, 176, 144)\n",
      "0.0\n",
      "1.0\n",
      "[0.0, 0.22465753424648938, 0.5424657534247217, 1.030136986301386, 2.05479452054783, 4.038356164383458, 5.076712328767144, 6.128767123287616, 7.15068493150693]\n"
     ]
    }
   ],
   "source": [
    "imgs,times =load_imgs_and_time(subject_path)\n",
    "#计算时间\n",
    "times = [cal_time(t)/365.0 for t in times]\n",
    "start_time = times[0]\n",
    "times = [t-start_time for t in times]\n",
    "\n",
    "print(len(imgs))\n",
    "print(imgs[0].shape)\n",
    "print(np.min(imgs[0]))\n",
    "print(np.max(imgs[0]))\n",
    "print(times)\n",
    "\n",
    "#示例subject002_S_4229的序列长度为9，这里我们按照  训练:测试=7:2 的比例 进行划分（默认是80%：20%，可根据不同subject进行调整）\n",
    "train_List = imgs[0:7]\n",
    "test_List =imgs[7:]\n",
    "train_times = times[0:7]\n",
    "test_times = times[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_shape =train_List[0].shape\n",
    "#numpy转tensor,增加batch和channel维度，方便后续输入到模型\n",
    "#144*176*144\n",
    "train_List = [torch.from_numpy(img).to(device).float().unsqueeze(0).unsqueeze(0) for img in train_List]\n",
    "test_List = [torch.from_numpy(img).to(device).float().unsqueeze(0).unsqueeze(0) for img in test_List]\n",
    "\n",
    "\n",
    "\n",
    "#定义网络v(这里采用简化的版本)\n",
    "Network = DynamicNet(img_sz=im_shape,\n",
    "                    smoothing_kernel='AK',\n",
    "                    smoothing_win=15,\n",
    "                    smoothing_pass=1,\n",
    "                    ds=2,\n",
    "                    bs=32\n",
    "                    ).to(device)\n",
    "\n",
    "#Network = UNet_3D_ebd(n_channels=4,n_classes=3,trilinear=False).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3241, device='cuda:0')\n",
      "tensor([0.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#模型的保存路径\n",
    "savePath = \"./model-save/002_S_4654\"\n",
    "#定义优化器\n",
    "optimizer = torch.optim.Adam(Network.parameters(), lr=0.005, amsgrad=True)\n",
    "epoches = 300\n",
    "\n",
    "# training loop\n",
    "scale_factor = torch.tensor(im_shape).to(device).view(1, 3, 1, 1, 1) * 1.\n",
    "ST = SpatialTransformer(im_shape).to(device)  # spatial transformer to warp image\n",
    "grid = generate_grid3D_tensor(im_shape).unsqueeze(0).to(device)  # [-1,1] 1*3*144*176*144 (identity map)\n",
    "\n",
    "#测试NCC计算\n",
    "loss_NCC = NCC(win=21)\n",
    "print(loss_NCC(train_List[0],train_List[1]))\n",
    "print(torch.tensor([0.0]).to(device))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#用于记录每个epoch的数据\n",
    "total_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3679: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 loss_NCC: 6.208e-01  loss_v: 3.268e-07 loss_J: 6.678e+00 loss_df: 1.752e-01 total_loss: 8.483e-01 time_cost: 1.332e+00 loss_MSE: 1.979e-02 loss_bdr: 5.240e-02 folding: 1.683e-01\n",
      "Iteration: 2 loss_NCC: 5.343e-01  loss_v: 1.743e-08 loss_J: 1.083e-01 loss_df: 1.807e-02 total_loss: 5.581e-01 time_cost: 1.285e+00 loss_MSE: 9.127e-03 loss_bdr: 5.660e-03 folding: 5.844e-02\n",
      "Iteration: 3 loss_NCC: 6.950e-01  loss_v: 3.326e-06 loss_J: 1.827e+02 loss_df: 1.599e+00 total_loss: 2.771e+00 time_cost: 1.286e+00 loss_MSE: 3.688e-02 loss_bdr: 4.765e-01 folding: 2.388e-01\n",
      "Iteration: 4 loss_NCC: 6.604e-01  loss_v: 1.200e-06 loss_J: 4.161e+01 loss_df: 5.963e-01 total_loss: 1.436e+00 time_cost: 1.289e+00 loss_MSE: 2.850e-02 loss_bdr: 1.798e-01 folding: 2.145e-01\n",
      "Iteration: 5 loss_NCC: 6.453e-01  loss_v: 1.138e-06 loss_J: 4.030e+01 loss_df: 5.860e-01 total_loss: 1.410e+00 time_cost: 1.291e+00 loss_MSE: 2.664e-02 loss_bdr: 1.789e-01 folding: 2.114e-01\n",
      "Iteration: 6 loss_NCC: 4.643e-01  loss_v: 1.593e-08 loss_J: 8.536e-02 loss_df: 1.561e-02 total_loss: 4.846e-01 time_cost: 1.293e+00 loss_MSE: 6.510e-03 loss_bdr: 4.683e-03 folding: 5.050e-02\n",
      "Iteration: 7 loss_NCC: 6.211e-01  loss_v: 5.598e-07 loss_J: 1.427e+01 loss_df: 2.911e-01 total_loss: 9.971e-01 time_cost: 1.295e+00 loss_MSE: 2.181e-02 loss_bdr: 8.499e-02 folding: 1.873e-01\n",
      "Iteration: 8 loss_NCC: 4.436e-01  loss_v: 1.406e-08 loss_J: 6.716e-02 loss_df: 1.423e-02 total_loss: 4.624e-01 time_cost: 1.291e+00 loss_MSE: 5.743e-03 loss_bdr: 4.551e-03 folding: 4.444e-02\n",
      "Iteration: 9 loss_NCC: 4.308e-01  loss_v: 1.383e-08 loss_J: 6.347e-02 loss_df: 1.391e-02 total_loss: 4.492e-01 time_cost: 1.285e+00 loss_MSE: 5.289e-03 loss_bdr: 4.545e-03 folding: 4.298e-02\n",
      "Iteration: 10 loss_NCC: 4.182e-01  loss_v: 1.349e-08 loss_J: 5.908e-02 loss_df: 1.352e-02 total_loss: 4.362e-01 time_cost: 1.292e+00 loss_MSE: 4.853e-03 loss_bdr: 4.510e-03 folding: 4.109e-02\n",
      "Iteration: 11 loss_NCC: 4.093e-01  loss_v: 1.313e-08 loss_J: 5.448e-02 loss_df: 1.310e-02 total_loss: 4.269e-01 time_cost: 1.290e+00 loss_MSE: 4.557e-03 loss_bdr: 4.446e-03 folding: 3.897e-02\n",
      "Iteration: 12 loss_NCC: 4.025e-01  loss_v: 1.272e-08 loss_J: 4.986e-02 loss_df: 1.266e-02 total_loss: 4.195e-01 time_cost: 1.284e+00 loss_MSE: 4.327e-03 loss_bdr: 4.349e-03 folding: 3.677e-02\n",
      "Iteration: 13 loss_NCC: 3.962e-01  loss_v: 1.225e-08 loss_J: 4.534e-02 loss_df: 1.222e-02 total_loss: 4.127e-01 time_cost: 1.290e+00 loss_MSE: 4.111e-03 loss_bdr: 4.218e-03 folding: 3.456e-02\n",
      "Iteration: 14 loss_NCC: 3.906e-01  loss_v: 1.172e-08 loss_J: 4.094e-02 loss_df: 1.177e-02 total_loss: 4.064e-01 time_cost: 1.294e+00 loss_MSE: 3.919e-03 loss_bdr: 4.053e-03 folding: 3.238e-02\n",
      "Iteration: 15 loss_NCC: 3.862e-01  loss_v: 1.115e-08 loss_J: 3.671e-02 loss_df: 1.132e-02 total_loss: 4.014e-01 time_cost: 1.285e+00 loss_MSE: 3.770e-03 loss_bdr: 3.860e-03 folding: 3.022e-02\n",
      "Iteration: 16 loss_NCC: 3.826e-01  loss_v: 1.056e-08 loss_J: 3.264e-02 loss_df: 1.087e-02 total_loss: 3.971e-01 time_cost: 1.286e+00 loss_MSE: 3.638e-03 loss_bdr: 3.645e-03 folding: 2.805e-02\n",
      "Iteration: 17 loss_NCC: 6.043e-01  loss_v: 4.560e-07 loss_J: 9.832e+00 loss_df: 2.264e-01 total_loss: 9.039e-01 time_cost: 1.286e+00 loss_MSE: 1.988e-02 loss_bdr: 7.319e-02 folding: 1.758e-01\n",
      "Iteration: 18 loss_NCC: 3.800e-01  loss_v: 9.514e-09 loss_J: 2.583e-02 loss_df: 1.006e-02 total_loss: 3.933e-01 time_cost: 1.288e+00 loss_MSE: 3.514e-03 loss_bdr: 3.194e-03 folding: 2.419e-02\n",
      "Iteration: 19 loss_NCC: 3.780e-01  loss_v: 9.049e-09 loss_J: 2.300e-02 loss_df: 9.697e-03 total_loss: 3.907e-01 time_cost: 1.289e+00 loss_MSE: 3.427e-03 loss_bdr: 2.983e-03 folding: 2.243e-02\n",
      "Iteration: 20 loss_NCC: 3.737e-01  loss_v: 8.549e-09 loss_J: 2.028e-02 loss_df: 9.313e-03 total_loss: 3.858e-01 time_cost: 1.278e+00 loss_MSE: 3.253e-03 loss_bdr: 2.778e-03 folding: 2.061e-02\n",
      "Iteration: 21 loss_NCC: 3.694e-01  loss_v: 8.047e-09 loss_J: 1.776e-02 loss_df: 8.925e-03 total_loss: 3.809e-01 time_cost: 1.287e+00 loss_MSE: 3.079e-03 loss_bdr: 2.582e-03 folding: 1.880e-02\n",
      "Iteration: 22 loss_NCC: 3.664e-01  loss_v: 7.560e-09 loss_J: 1.547e-02 loss_df: 8.543e-03 total_loss: 3.773e-01 time_cost: 1.283e+00 loss_MSE: 2.943e-03 loss_bdr: 2.393e-03 folding: 1.705e-02\n",
      "Iteration: 23 loss_NCC: 3.640e-01  loss_v: 7.083e-09 loss_J: 1.342e-02 loss_df: 8.170e-03 total_loss: 3.744e-01 time_cost: 1.284e+00 loss_MSE: 2.832e-03 loss_bdr: 2.214e-03 folding: 1.540e-02\n",
      "Iteration: 24 loss_NCC: 3.617e-01  loss_v: 6.623e-09 loss_J: 1.162e-02 loss_df: 7.817e-03 total_loss: 3.715e-01 time_cost: 1.291e+00 loss_MSE: 2.735e-03 loss_bdr: 2.046e-03 folding: 1.386e-02\n",
      "Iteration: 25 loss_NCC: 3.594e-01  loss_v: 6.169e-09 loss_J: 9.983e-03 loss_df: 7.467e-03 total_loss: 3.688e-01 time_cost: 1.290e+00 loss_MSE: 2.647e-03 loss_bdr: 1.885e-03 folding: 1.242e-02\n",
      "Iteration: 26 loss_NCC: 3.572e-01  loss_v: 5.762e-09 loss_J: 8.609e-03 loss_df: 7.152e-03 total_loss: 3.661e-01 time_cost: 1.290e+00 loss_MSE: 2.572e-03 loss_bdr: 1.740e-03 folding: 1.114e-02\n",
      "Iteration: 27 loss_NCC: 3.554e-01  loss_v: 5.377e-09 loss_J: 7.399e-03 loss_df: 6.850e-03 total_loss: 3.639e-01 time_cost: 1.288e+00 loss_MSE: 2.503e-03 loss_bdr: 1.603e-03 folding: 9.959e-03\n",
      "Iteration: 28 loss_NCC: 3.536e-01  loss_v: 5.013e-09 loss_J: 6.342e-03 loss_df: 6.563e-03 total_loss: 3.617e-01 time_cost: 1.289e+00 loss_MSE: 2.430e-03 loss_bdr: 1.477e-03 folding: 8.869e-03\n",
      "Iteration: 29 loss_NCC: 3.520e-01  loss_v: 4.671e-09 loss_J: 5.421e-03 loss_df: 6.291e-03 total_loss: 3.596e-01 time_cost: 1.288e+00 loss_MSE: 2.360e-03 loss_bdr: 1.360e-03 folding: 7.856e-03\n",
      "Iteration: 30 loss_NCC: 3.504e-01  loss_v: 4.350e-09 loss_J: 4.620e-03 loss_df: 6.035e-03 total_loss: 3.577e-01 time_cost: 1.288e+00 loss_MSE: 2.290e-03 loss_bdr: 1.254e-03 folding: 6.933e-03\n",
      "Iteration: 31 loss_NCC: 3.489e-01  loss_v: 4.050e-09 loss_J: 3.927e-03 loss_df: 5.794e-03 total_loss: 3.559e-01 time_cost: 1.288e+00 loss_MSE: 2.227e-03 loss_bdr: 1.158e-03 folding: 6.104e-03\n",
      "Iteration: 32 loss_NCC: 3.476e-01  loss_v: 3.774e-09 loss_J: 3.335e-03 loss_df: 5.568e-03 total_loss: 3.543e-01 time_cost: 1.289e+00 loss_MSE: 2.175e-03 loss_bdr: 1.071e-03 folding: 5.359e-03\n",
      "Iteration: 33 loss_NCC: 3.464e-01  loss_v: 3.526e-09 loss_J: 2.835e-03 loss_df: 5.358e-03 total_loss: 3.527e-01 time_cost: 1.285e+00 loss_MSE: 2.131e-03 loss_bdr: 9.938e-04 folding: 4.705e-03\n",
      "Iteration: 34 loss_NCC: 3.452e-01  loss_v: 3.306e-09 loss_J: 2.416e-03 loss_df: 5.163e-03 total_loss: 3.513e-01 time_cost: 1.291e+00 loss_MSE: 2.086e-03 loss_bdr: 9.247e-04 folding: 4.142e-03\n",
      "Iteration: 35 loss_NCC: 3.440e-01  loss_v: 3.113e-09 loss_J: 2.068e-03 loss_df: 4.983e-03 total_loss: 3.498e-01 time_cost: 1.290e+00 loss_MSE: 2.042e-03 loss_bdr: 8.630e-04 folding: 3.666e-03\n",
      "Iteration: 36 loss_NCC: 3.429e-01  loss_v: 2.941e-09 loss_J: 1.777e-03 loss_df: 4.816e-03 total_loss: 3.486e-01 time_cost: 1.291e+00 loss_MSE: 2.004e-03 loss_bdr: 8.081e-04 folding: 3.243e-03\n",
      "Iteration: 37 loss_NCC: 3.419e-01  loss_v: 2.783e-09 loss_J: 1.532e-03 loss_df: 4.661e-03 total_loss: 3.474e-01 time_cost: 1.290e+00 loss_MSE: 1.974e-03 loss_bdr: 7.591e-04 folding: 2.873e-03\n",
      "Iteration: 38 loss_NCC: 3.410e-01  loss_v: 2.639e-09 loss_J: 1.322e-03 loss_df: 4.516e-03 total_loss: 3.462e-01 time_cost: 1.287e+00 loss_MSE: 1.950e-03 loss_bdr: 7.153e-04 folding: 2.554e-03\n",
      "Iteration: 39 loss_NCC: 3.400e-01  loss_v: 2.509e-09 loss_J: 1.143e-03 loss_df: 4.380e-03 total_loss: 3.451e-01 time_cost: 1.283e+00 loss_MSE: 1.924e-03 loss_bdr: 6.758e-04 folding: 2.270e-03\n",
      "Iteration: 40 loss_NCC: 3.392e-01  loss_v: 2.395e-09 loss_J: 9.921e-04 loss_df: 4.252e-03 total_loss: 3.441e-01 time_cost: 1.285e+00 loss_MSE: 1.896e-03 loss_bdr: 6.402e-04 folding: 2.027e-03\n",
      "Iteration: 41 loss_NCC: 3.384e-01  loss_v: 2.295e-09 loss_J: 8.665e-04 loss_df: 4.131e-03 total_loss: 3.431e-01 time_cost: 1.285e+00 loss_MSE: 1.867e-03 loss_bdr: 6.081e-04 folding: 1.811e-03\n",
      "Iteration: 42 loss_NCC: 3.376e-01  loss_v: 2.204e-09 loss_J: 7.638e-04 loss_df: 4.018e-03 total_loss: 3.422e-01 time_cost: 1.283e+00 loss_MSE: 1.840e-03 loss_bdr: 5.793e-04 folding: 1.625e-03\n",
      "Iteration: 43 loss_NCC: 3.369e-01  loss_v: 2.118e-09 loss_J: 6.784e-04 loss_df: 3.910e-03 total_loss: 3.414e-01 time_cost: 1.287e+00 loss_MSE: 1.820e-03 loss_bdr: 5.531e-04 folding: 1.468e-03\n",
      "Iteration: 44 loss_NCC: 3.363e-01  loss_v: 2.036e-09 loss_J: 6.052e-04 loss_df: 3.809e-03 total_loss: 3.406e-01 time_cost: 1.283e+00 loss_MSE: 1.805e-03 loss_bdr: 5.291e-04 folding: 1.326e-03\n",
      "Iteration: 45 loss_NCC: 3.356e-01  loss_v: 1.960e-09 loss_J: 5.407e-04 loss_df: 3.713e-03 total_loss: 3.399e-01 time_cost: 1.286e+00 loss_MSE: 1.791e-03 loss_bdr: 5.070e-04 folding: 1.206e-03\n",
      "Iteration: 46 loss_NCC: 3.350e-01  loss_v: 1.891e-09 loss_J: 4.827e-04 loss_df: 3.622e-03 total_loss: 3.391e-01 time_cost: 1.287e+00 loss_MSE: 1.776e-03 loss_bdr: 4.869e-04 folding: 1.092e-03\n",
      "Iteration: 47 loss_NCC: 3.345e-01  loss_v: 1.827e-09 loss_J: 4.313e-04 loss_df: 3.537e-03 total_loss: 3.385e-01 time_cost: 1.287e+00 loss_MSE: 1.759e-03 loss_bdr: 4.690e-04 folding: 9.970e-04\n",
      "Iteration: 48 loss_NCC: 3.340e-01  loss_v: 1.769e-09 loss_J: 3.859e-04 loss_df: 3.455e-03 total_loss: 3.379e-01 time_cost: 1.285e+00 loss_MSE: 1.742e-03 loss_bdr: 4.533e-04 folding: 9.112e-04\n",
      "Iteration: 49 loss_NCC: 3.335e-01  loss_v: 1.705e-09 loss_J: 3.457e-04 loss_df: 3.376e-03 total_loss: 3.373e-01 time_cost: 1.285e+00 loss_MSE: 1.726e-03 loss_bdr: 4.384e-04 folding: 8.297e-04\n",
      "Iteration: 50 loss_NCC: 3.331e-01  loss_v: 1.673e-09 loss_J: 3.107e-04 loss_df: 3.304e-03 total_loss: 3.369e-01 time_cost: 1.286e+00 loss_MSE: 1.718e-03 loss_bdr: 4.258e-04 folding: 7.598e-04\n",
      "Iteration: 51 loss_NCC: 3.326e-01  loss_v: 1.618e-09 loss_J: 2.787e-04 loss_df: 3.231e-03 total_loss: 3.362e-01 time_cost: 1.258e+00 loss_MSE: 1.703e-03 loss_bdr: 4.114e-04 folding: 6.937e-04\n",
      "Iteration: 52 loss_NCC: 3.329e-01  loss_v: 1.537e-09 loss_J: 2.489e-04 loss_df: 3.157e-03 total_loss: 3.364e-01 time_cost: 1.249e+00 loss_MSE: 1.703e-03 loss_bdr: 3.955e-04 folding: 6.328e-04\n",
      "Iteration: 53 loss_NCC: 3.325e-01  loss_v: 1.551e-09 loss_J: 2.227e-04 loss_df: 3.099e-03 total_loss: 3.360e-01 time_cost: 1.253e+00 loss_MSE: 1.707e-03 loss_bdr: 3.858e-04 folding: 5.789e-04\n",
      "Iteration: 54 loss_NCC: 3.350e-01  loss_v: 1.607e-09 loss_J: 2.045e-04 loss_df: 3.062e-03 total_loss: 3.384e-01 time_cost: 1.254e+00 loss_MSE: 1.783e-03 loss_bdr: 3.825e-04 folding: 5.356e-04\n",
      "Iteration: 55 loss_NCC: 3.334e-01  loss_v: 1.474e-09 loss_J: 1.782e-04 loss_df: 2.975e-03 total_loss: 3.368e-01 time_cost: 1.253e+00 loss_MSE: 1.715e-03 loss_bdr: 3.636e-04 folding: 4.817e-04\n",
      "Iteration: 56 loss_NCC: 3.327e-01  loss_v: 1.428e-09 loss_J: 1.581e-04 loss_df: 2.915e-03 total_loss: 3.360e-01 time_cost: 1.257e+00 loss_MSE: 1.699e-03 loss_bdr: 3.526e-04 folding: 4.355e-04\n",
      "Iteration: 57 loss_NCC: 3.314e-01  loss_v: 1.398e-09 loss_J: 1.404e-04 loss_df: 2.857e-03 total_loss: 3.346e-01 time_cost: 1.251e+00 loss_MSE: 1.672e-03 loss_bdr: 3.421e-04 folding: 3.946e-04\n",
      "Iteration: 58 loss_NCC: 3.320e-01  loss_v: 1.388e-09 loss_J: 1.275e-04 loss_df: 2.805e-03 total_loss: 3.351e-01 time_cost: 1.252e+00 loss_MSE: 1.688e-03 loss_bdr: 3.329e-04 folding: 3.636e-04\n",
      "Iteration: 59 loss_NCC: 3.309e-01  loss_v: 1.352e-09 loss_J: 1.176e-04 loss_df: 2.755e-03 total_loss: 3.340e-01 time_cost: 1.252e+00 loss_MSE: 1.653e-03 loss_bdr: 3.245e-04 folding: 3.360e-04\n",
      "Iteration: 60 loss_NCC: 3.308e-01  loss_v: 1.311e-09 loss_J: 1.087e-04 loss_df: 2.706e-03 total_loss: 3.338e-01 time_cost: 1.251e+00 loss_MSE: 1.646e-03 loss_bdr: 3.167e-04 folding: 3.126e-04\n",
      "Iteration: 61 loss_NCC: 3.306e-01  loss_v: 1.277e-09 loss_J: 9.957e-05 loss_df: 2.660e-03 total_loss: 3.336e-01 time_cost: 1.253e+00 loss_MSE: 1.644e-03 loss_bdr: 3.090e-04 folding: 2.882e-04\n",
      "Iteration: 62 loss_NCC: 3.301e-01  loss_v: 1.258e-09 loss_J: 9.031e-05 loss_df: 2.615e-03 total_loss: 3.330e-01 time_cost: 1.251e+00 loss_MSE: 1.632e-03 loss_bdr: 3.016e-04 folding: 2.649e-04\n",
      "Iteration: 63 loss_NCC: 3.300e-01  loss_v: 1.249e-09 loss_J: 8.183e-05 loss_df: 2.572e-03 total_loss: 3.329e-01 time_cost: 1.251e+00 loss_MSE: 1.629e-03 loss_bdr: 2.946e-04 folding: 2.427e-04\n",
      "Iteration: 64 loss_NCC: 3.296e-01  loss_v: 1.231e-09 loss_J: 7.453e-05 loss_df: 2.531e-03 total_loss: 3.324e-01 time_cost: 1.251e+00 loss_MSE: 1.613e-03 loss_bdr: 2.879e-04 folding: 2.236e-04\n",
      "Iteration: 65 loss_NCC: 3.294e-01  loss_v: 1.206e-09 loss_J: 6.866e-05 loss_df: 2.490e-03 total_loss: 3.322e-01 time_cost: 1.250e+00 loss_MSE: 1.607e-03 loss_bdr: 2.817e-04 folding: 2.080e-04\n",
      "Iteration: 66 loss_NCC: 3.292e-01  loss_v: 1.182e-09 loss_J: 6.389e-05 loss_df: 2.452e-03 total_loss: 3.319e-01 time_cost: 1.253e+00 loss_MSE: 1.605e-03 loss_bdr: 2.760e-04 folding: 1.947e-04\n",
      "Iteration: 67 loss_NCC: 3.289e-01  loss_v: 1.165e-09 loss_J: 5.961e-05 loss_df: 2.414e-03 total_loss: 3.316e-01 time_cost: 1.253e+00 loss_MSE: 1.598e-03 loss_bdr: 2.707e-04 folding: 1.830e-04\n",
      "Iteration: 68 loss_NCC: 3.287e-01  loss_v: 1.157e-09 loss_J: 5.528e-05 loss_df: 2.378e-03 total_loss: 3.314e-01 time_cost: 1.261e+00 loss_MSE: 1.588e-03 loss_bdr: 2.659e-04 folding: 1.713e-04\n",
      "Iteration: 69 loss_NCC: 3.285e-01  loss_v: 1.148e-09 loss_J: 5.074e-05 loss_df: 2.343e-03 total_loss: 3.312e-01 time_cost: 1.254e+00 loss_MSE: 1.578e-03 loss_bdr: 2.611e-04 folding: 1.591e-04\n",
      "Iteration: 70 loss_NCC: 3.283e-01  loss_v: 1.134e-09 loss_J: 4.672e-05 loss_df: 2.308e-03 total_loss: 3.309e-01 time_cost: 1.256e+00 loss_MSE: 1.574e-03 loss_bdr: 2.564e-04 folding: 1.489e-04\n",
      "Iteration: 71 loss_NCC: 3.282e-01  loss_v: 1.117e-09 loss_J: 4.382e-05 loss_df: 2.275e-03 total_loss: 3.307e-01 time_cost: 1.253e+00 loss_MSE: 1.573e-03 loss_bdr: 2.518e-04 folding: 1.408e-04\n",
      "Iteration: 72 loss_NCC: 3.280e-01  loss_v: 1.100e-09 loss_J: 4.210e-05 loss_df: 2.244e-03 total_loss: 3.305e-01 time_cost: 1.251e+00 loss_MSE: 1.569e-03 loss_bdr: 2.474e-04 folding: 1.352e-04\n",
      "Iteration: 73 loss_NCC: 3.278e-01  loss_v: 1.090e-09 loss_J: 4.116e-05 loss_df: 2.214e-03 total_loss: 3.303e-01 time_cost: 1.250e+00 loss_MSE: 1.562e-03 loss_bdr: 2.431e-04 folding: 1.294e-04\n",
      "Iteration: 74 loss_NCC: 3.277e-01  loss_v: 1.085e-09 loss_J: 3.995e-05 loss_df: 2.184e-03 total_loss: 3.301e-01 time_cost: 1.263e+00 loss_MSE: 1.555e-03 loss_bdr: 2.389e-04 folding: 1.235e-04\n",
      "Iteration: 75 loss_NCC: 3.275e-01  loss_v: 1.077e-09 loss_J: 3.777e-05 loss_df: 2.155e-03 total_loss: 3.299e-01 time_cost: 1.256e+00 loss_MSE: 1.552e-03 loss_bdr: 2.347e-04 folding: 1.167e-04\n",
      "Iteration: 76 loss_NCC: 3.274e-01  loss_v: 1.061e-09 loss_J: 3.510e-05 loss_df: 2.127e-03 total_loss: 3.297e-01 time_cost: 1.257e+00 loss_MSE: 1.552e-03 loss_bdr: 2.304e-04 folding: 1.097e-04\n",
      "Iteration: 77 loss_NCC: 3.273e-01  loss_v: 1.044e-09 loss_J: 3.294e-05 loss_df: 2.099e-03 total_loss: 3.296e-01 time_cost: 1.255e+00 loss_MSE: 1.550e-03 loss_bdr: 2.264e-04 folding: 1.034e-04\n",
      "Iteration: 78 loss_NCC: 3.271e-01  loss_v: 1.034e-09 loss_J: 3.173e-05 loss_df: 2.073e-03 total_loss: 3.294e-01 time_cost: 1.253e+00 loss_MSE: 1.543e-03 loss_bdr: 2.227e-04 folding: 9.790e-05\n",
      "Iteration: 79 loss_NCC: 3.270e-01  loss_v: 1.030e-09 loss_J: 3.084e-05 loss_df: 2.047e-03 total_loss: 3.293e-01 time_cost: 1.257e+00 loss_MSE: 1.537e-03 loss_bdr: 2.192e-04 folding: 9.417e-05\n",
      "Iteration: 80 loss_NCC: 3.269e-01  loss_v: 1.022e-09 loss_J: 2.981e-05 loss_df: 2.022e-03 total_loss: 3.291e-01 time_cost: 1.256e+00 loss_MSE: 1.536e-03 loss_bdr: 2.158e-04 folding: 9.091e-05\n",
      "Iteration: 81 loss_NCC: 3.268e-01  loss_v: 1.008e-09 loss_J: 2.886e-05 loss_df: 1.998e-03 total_loss: 3.290e-01 time_cost: 1.255e+00 loss_MSE: 1.534e-03 loss_bdr: 2.124e-04 folding: 8.784e-05\n",
      "Iteration: 82 loss_NCC: 3.267e-01  loss_v: 9.954e-10 loss_J: 2.818e-05 loss_df: 1.976e-03 total_loss: 3.289e-01 time_cost: 1.258e+00 loss_MSE: 1.530e-03 loss_bdr: 2.092e-04 folding: 8.588e-05\n",
      "Iteration: 83 loss_NCC: 3.266e-01  loss_v: 9.887e-10 loss_J: 2.770e-05 loss_df: 1.954e-03 total_loss: 3.287e-01 time_cost: 1.255e+00 loss_MSE: 1.525e-03 loss_bdr: 2.063e-04 folding: 8.346e-05\n",
      "Iteration: 84 loss_NCC: 3.264e-01  loss_v: 9.863e-10 loss_J: 2.722e-05 loss_df: 1.932e-03 total_loss: 3.286e-01 time_cost: 1.250e+00 loss_MSE: 1.523e-03 loss_bdr: 2.037e-04 folding: 8.094e-05\n",
      "Iteration: 85 loss_NCC: 3.264e-01  loss_v: 9.895e-10 loss_J: 2.657e-05 loss_df: 1.913e-03 total_loss: 3.285e-01 time_cost: 1.252e+00 loss_MSE: 1.522e-03 loss_bdr: 2.016e-04 folding: 7.913e-05\n",
      "Iteration: 86 loss_NCC: 3.263e-01  loss_v: 9.662e-10 loss_J: 2.593e-05 loss_df: 1.890e-03 total_loss: 3.284e-01 time_cost: 1.252e+00 loss_MSE: 1.518e-03 loss_bdr: 1.979e-04 folding: 7.680e-05\n",
      "Iteration: 87 loss_NCC: 3.262e-01  loss_v: 9.855e-10 loss_J: 2.530e-05 loss_df: 1.875e-03 total_loss: 3.283e-01 time_cost: 1.253e+00 loss_MSE: 1.517e-03 loss_bdr: 1.965e-04 folding: 7.428e-05\n",
      "Iteration: 88 loss_NCC: 3.261e-01  loss_v: 9.601e-10 loss_J: 2.530e-05 loss_df: 1.851e-03 total_loss: 3.281e-01 time_cost: 1.250e+00 loss_MSE: 1.512e-03 loss_bdr: 1.928e-04 folding: 7.284e-05\n",
      "Iteration: 89 loss_NCC: 3.260e-01  loss_v: 9.735e-10 loss_J: 2.511e-05 loss_df: 1.835e-03 total_loss: 3.280e-01 time_cost: 1.250e+00 loss_MSE: 1.511e-03 loss_bdr: 1.914e-04 folding: 7.028e-05\n",
      "Iteration: 90 loss_NCC: 3.259e-01  loss_v: 9.612e-10 loss_J: 2.445e-05 loss_df: 1.816e-03 total_loss: 3.279e-01 time_cost: 1.249e+00 loss_MSE: 1.509e-03 loss_bdr: 1.885e-04 folding: 6.800e-05\n",
      "Iteration: 91 loss_NCC: 3.258e-01  loss_v: 9.500e-10 loss_J: 2.338e-05 loss_df: 1.797e-03 total_loss: 3.278e-01 time_cost: 1.252e+00 loss_MSE: 1.506e-03 loss_bdr: 1.857e-04 folding: 6.530e-05\n",
      "Iteration: 92 loss_NCC: 3.257e-01  loss_v: 9.609e-10 loss_J: 2.245e-05 loss_df: 1.783e-03 total_loss: 3.277e-01 time_cost: 1.247e+00 loss_MSE: 1.504e-03 loss_bdr: 1.844e-04 folding: 6.320e-05\n",
      "Iteration: 93 loss_NCC: 3.256e-01  loss_v: 9.436e-10 loss_J: 2.238e-05 loss_df: 1.763e-03 total_loss: 3.276e-01 time_cost: 1.249e+00 loss_MSE: 1.501e-03 loss_bdr: 1.817e-04 folding: 6.213e-05\n",
      "Iteration: 94 loss_NCC: 3.256e-01  loss_v: 9.501e-10 loss_J: 2.208e-05 loss_df: 1.748e-03 total_loss: 3.275e-01 time_cost: 1.248e+00 loss_MSE: 1.500e-03 loss_bdr: 1.804e-04 folding: 6.087e-05\n",
      "Iteration: 95 loss_NCC: 3.255e-01  loss_v: 9.470e-10 loss_J: 2.178e-05 loss_df: 1.733e-03 total_loss: 3.274e-01 time_cost: 1.248e+00 loss_MSE: 1.499e-03 loss_bdr: 1.783e-04 folding: 5.961e-05\n",
      "Iteration: 96 loss_NCC: 3.254e-01  loss_v: 9.354e-10 loss_J: 2.163e-05 loss_df: 1.715e-03 total_loss: 3.273e-01 time_cost: 1.246e+00 loss_MSE: 1.496e-03 loss_bdr: 1.758e-04 folding: 5.854e-05\n",
      "Iteration: 97 loss_NCC: 3.253e-01  loss_v: 9.468e-10 loss_J: 2.132e-05 loss_df: 1.702e-03 total_loss: 3.272e-01 time_cost: 1.245e+00 loss_MSE: 1.494e-03 loss_bdr: 1.747e-04 folding: 5.798e-05\n",
      "Iteration: 98 loss_NCC: 3.253e-01  loss_v: 9.383e-10 loss_J: 2.122e-05 loss_df: 1.686e-03 total_loss: 3.271e-01 time_cost: 1.249e+00 loss_MSE: 1.491e-03 loss_bdr: 1.724e-04 folding: 5.673e-05\n",
      "Iteration: 99 loss_NCC: 3.252e-01  loss_v: 9.333e-10 loss_J: 2.099e-05 loss_df: 1.671e-03 total_loss: 3.270e-01 time_cost: 1.250e+00 loss_MSE: 1.490e-03 loss_bdr: 1.703e-04 folding: 5.524e-05\n",
      "Iteration: 100 loss_NCC: 3.251e-01  loss_v: 9.414e-10 loss_J: 2.055e-05 loss_df: 1.658e-03 total_loss: 3.269e-01 time_cost: 1.247e+00 loss_MSE: 1.489e-03 loss_bdr: 1.689e-04 folding: 5.407e-05\n",
      "Iteration: 101 loss_NCC: 3.250e-01  loss_v: 9.341e-10 loss_J: 2.023e-05 loss_df: 1.643e-03 total_loss: 3.269e-01 time_cost: 1.249e+00 loss_MSE: 1.486e-03 loss_bdr: 1.668e-04 folding: 5.258e-05\n",
      "Iteration: 102 loss_NCC: 3.250e-01  loss_v: 9.347e-10 loss_J: 1.957e-05 loss_df: 1.630e-03 total_loss: 3.268e-01 time_cost: 1.253e+00 loss_MSE: 1.484e-03 loss_bdr: 1.652e-04 folding: 5.095e-05\n",
      "Iteration: 103 loss_NCC: 3.249e-01  loss_v: 9.376e-10 loss_J: 1.891e-05 loss_df: 1.617e-03 total_loss: 3.267e-01 time_cost: 1.251e+00 loss_MSE: 1.483e-03 loss_bdr: 1.637e-04 folding: 4.946e-05\n",
      "Iteration: 104 loss_NCC: 3.248e-01  loss_v: 9.320e-10 loss_J: 1.888e-05 loss_df: 1.603e-03 total_loss: 3.266e-01 time_cost: 1.256e+00 loss_MSE: 1.481e-03 loss_bdr: 1.617e-04 folding: 4.820e-05\n",
      "Iteration: 105 loss_NCC: 3.248e-01  loss_v: 9.355e-10 loss_J: 1.881e-05 loss_df: 1.591e-03 total_loss: 3.265e-01 time_cost: 1.249e+00 loss_MSE: 1.480e-03 loss_bdr: 1.603e-04 folding: 4.760e-05\n",
      "Iteration: 106 loss_NCC: 3.247e-01  loss_v: 9.369e-10 loss_J: 1.836e-05 loss_df: 1.580e-03 total_loss: 3.264e-01 time_cost: 1.251e+00 loss_MSE: 1.478e-03 loss_bdr: 1.589e-04 folding: 4.611e-05\n",
      "Iteration: 107 loss_NCC: 3.246e-01  loss_v: 9.336e-10 loss_J: 1.788e-05 loss_df: 1.567e-03 total_loss: 3.264e-01 time_cost: 1.252e+00 loss_MSE: 1.477e-03 loss_bdr: 1.572e-04 folding: 4.518e-05\n",
      "Iteration: 108 loss_NCC: 3.246e-01  loss_v: 9.382e-10 loss_J: 1.774e-05 loss_df: 1.556e-03 total_loss: 3.263e-01 time_cost: 1.249e+00 loss_MSE: 1.476e-03 loss_bdr: 1.559e-04 folding: 4.485e-05\n",
      "Iteration: 109 loss_NCC: 3.245e-01  loss_v: 9.380e-10 loss_J: 1.782e-05 loss_df: 1.545e-03 total_loss: 3.262e-01 time_cost: 1.247e+00 loss_MSE: 1.474e-03 loss_bdr: 1.543e-04 folding: 4.494e-05\n",
      "Iteration: 110 loss_NCC: 3.245e-01  loss_v: 9.343e-10 loss_J: 1.758e-05 loss_df: 1.533e-03 total_loss: 3.261e-01 time_cost: 1.246e+00 loss_MSE: 1.472e-03 loss_bdr: 1.526e-04 folding: 4.424e-05\n",
      "Iteration: 111 loss_NCC: 3.244e-01  loss_v: 9.393e-10 loss_J: 1.706e-05 loss_df: 1.523e-03 total_loss: 3.261e-01 time_cost: 1.248e+00 loss_MSE: 1.472e-03 loss_bdr: 1.514e-04 folding: 4.303e-05\n",
      "Iteration: 112 loss_NCC: 3.243e-01  loss_v: 9.417e-10 loss_J: 1.703e-05 loss_df: 1.512e-03 total_loss: 3.260e-01 time_cost: 1.250e+00 loss_MSE: 1.471e-03 loss_bdr: 1.501e-04 folding: 4.215e-05\n",
      "Iteration: 113 loss_NCC: 3.243e-01  loss_v: 9.413e-10 loss_J: 1.729e-05 loss_df: 1.501e-03 total_loss: 3.259e-01 time_cost: 1.248e+00 loss_MSE: 1.469e-03 loss_bdr: 1.486e-04 folding: 4.261e-05\n",
      "Iteration: 114 loss_NCC: 3.242e-01  loss_v: 9.445e-10 loss_J: 1.726e-05 loss_df: 1.491e-03 total_loss: 3.259e-01 time_cost: 1.250e+00 loss_MSE: 1.468e-03 loss_bdr: 1.474e-04 folding: 4.210e-05\n",
      "Iteration: 115 loss_NCC: 3.242e-01  loss_v: 9.454e-10 loss_J: 1.710e-05 loss_df: 1.481e-03 total_loss: 3.258e-01 time_cost: 1.254e+00 loss_MSE: 1.467e-03 loss_bdr: 1.461e-04 folding: 4.168e-05\n",
      "Iteration: 116 loss_NCC: 3.241e-01  loss_v: 9.473e-10 loss_J: 1.707e-05 loss_df: 1.472e-03 total_loss: 3.257e-01 time_cost: 1.249e+00 loss_MSE: 1.466e-03 loss_bdr: 1.449e-04 folding: 4.140e-05\n",
      "Iteration: 117 loss_NCC: 3.241e-01  loss_v: 9.512e-10 loss_J: 1.710e-05 loss_df: 1.463e-03 total_loss: 3.257e-01 time_cost: 1.248e+00 loss_MSE: 1.464e-03 loss_bdr: 1.438e-04 folding: 4.140e-05\n",
      "Iteration: 118 loss_NCC: 3.240e-01  loss_v: 9.525e-10 loss_J: 1.704e-05 loss_df: 1.453e-03 total_loss: 3.256e-01 time_cost: 1.249e+00 loss_MSE: 1.463e-03 loss_bdr: 1.425e-04 folding: 4.112e-05\n",
      "Iteration: 119 loss_NCC: 3.239e-01  loss_v: 9.547e-10 loss_J: 1.682e-05 loss_df: 1.444e-03 total_loss: 3.255e-01 time_cost: 1.251e+00 loss_MSE: 1.462e-03 loss_bdr: 1.414e-04 folding: 4.075e-05\n",
      "Iteration: 120 loss_NCC: 3.239e-01  loss_v: 9.606e-10 loss_J: 1.662e-05 loss_df: 1.436e-03 total_loss: 3.255e-01 time_cost: 1.250e+00 loss_MSE: 1.461e-03 loss_bdr: 1.405e-04 folding: 4.001e-05\n",
      "Iteration: 121 loss_NCC: 3.238e-01  loss_v: 9.635e-10 loss_J: 1.653e-05 loss_df: 1.427e-03 total_loss: 3.254e-01 time_cost: 1.248e+00 loss_MSE: 1.459e-03 loss_bdr: 1.394e-04 folding: 4.005e-05\n",
      "Iteration: 122 loss_NCC: 3.238e-01  loss_v: 9.660e-10 loss_J: 1.645e-05 loss_df: 1.418e-03 total_loss: 3.253e-01 time_cost: 1.253e+00 loss_MSE: 1.459e-03 loss_bdr: 1.383e-04 folding: 4.005e-05\n",
      "Iteration: 123 loss_NCC: 3.237e-01  loss_v: 9.708e-10 loss_J: 1.638e-05 loss_df: 1.410e-03 total_loss: 3.253e-01 time_cost: 1.254e+00 loss_MSE: 1.457e-03 loss_bdr: 1.373e-04 folding: 3.987e-05\n",
      "Iteration: 124 loss_NCC: 3.237e-01  loss_v: 9.751e-10 loss_J: 1.639e-05 loss_df: 1.402e-03 total_loss: 3.252e-01 time_cost: 1.251e+00 loss_MSE: 1.457e-03 loss_bdr: 1.363e-04 folding: 3.991e-05\n",
      "Iteration: 125 loss_NCC: 3.236e-01  loss_v: 9.779e-10 loss_J: 1.646e-05 loss_df: 1.394e-03 total_loss: 3.252e-01 time_cost: 1.252e+00 loss_MSE: 1.456e-03 loss_bdr: 1.353e-04 folding: 3.987e-05\n",
      "Iteration: 126 loss_NCC: 3.236e-01  loss_v: 9.817e-10 loss_J: 1.651e-05 loss_df: 1.387e-03 total_loss: 3.251e-01 time_cost: 1.249e+00 loss_MSE: 1.455e-03 loss_bdr: 1.343e-04 folding: 3.968e-05\n",
      "Iteration: 127 loss_NCC: 3.235e-01  loss_v: 9.857e-10 loss_J: 1.663e-05 loss_df: 1.379e-03 total_loss: 3.250e-01 time_cost: 1.252e+00 loss_MSE: 1.453e-03 loss_bdr: 1.334e-04 folding: 3.940e-05\n",
      "Iteration: 128 loss_NCC: 3.235e-01  loss_v: 9.892e-10 loss_J: 1.669e-05 loss_df: 1.372e-03 total_loss: 3.250e-01 time_cost: 1.250e+00 loss_MSE: 1.453e-03 loss_bdr: 1.324e-04 folding: 3.898e-05\n",
      "Iteration: 129 loss_NCC: 3.234e-01  loss_v: 9.932e-10 loss_J: 1.678e-05 loss_df: 1.364e-03 total_loss: 3.249e-01 time_cost: 1.248e+00 loss_MSE: 1.451e-03 loss_bdr: 1.314e-04 folding: 3.894e-05\n",
      "Iteration: 130 loss_NCC: 3.234e-01  loss_v: 9.965e-10 loss_J: 1.698e-05 loss_df: 1.357e-03 total_loss: 3.249e-01 time_cost: 1.249e+00 loss_MSE: 1.451e-03 loss_bdr: 1.305e-04 folding: 3.889e-05\n",
      "Iteration: 131 loss_NCC: 3.234e-01  loss_v: 1.001e-09 loss_J: 1.720e-05 loss_df: 1.350e-03 total_loss: 3.248e-01 time_cost: 1.253e+00 loss_MSE: 1.449e-03 loss_bdr: 1.295e-04 folding: 3.884e-05\n",
      "Iteration: 132 loss_NCC: 3.233e-01  loss_v: 1.004e-09 loss_J: 1.722e-05 loss_df: 1.343e-03 total_loss: 3.248e-01 time_cost: 1.247e+00 loss_MSE: 1.450e-03 loss_bdr: 1.285e-04 folding: 3.880e-05\n",
      "Iteration: 133 loss_NCC: 3.233e-01  loss_v: 1.008e-09 loss_J: 1.745e-05 loss_df: 1.337e-03 total_loss: 3.247e-01 time_cost: 1.250e+00 loss_MSE: 1.447e-03 loss_bdr: 1.276e-04 folding: 3.894e-05\n",
      "Iteration: 134 loss_NCC: 3.232e-01  loss_v: 1.011e-09 loss_J: 1.759e-05 loss_df: 1.330e-03 total_loss: 3.247e-01 time_cost: 1.249e+00 loss_MSE: 1.448e-03 loss_bdr: 1.266e-04 folding: 3.889e-05\n",
      "Iteration: 135 loss_NCC: 3.232e-01  loss_v: 1.015e-09 loss_J: 1.769e-05 loss_df: 1.324e-03 total_loss: 3.246e-01 time_cost: 1.250e+00 loss_MSE: 1.445e-03 loss_bdr: 1.257e-04 folding: 3.884e-05\n",
      "Iteration: 136 loss_NCC: 3.231e-01  loss_v: 1.018e-09 loss_J: 1.785e-05 loss_df: 1.317e-03 total_loss: 3.246e-01 time_cost: 1.255e+00 loss_MSE: 1.446e-03 loss_bdr: 1.247e-04 folding: 3.894e-05\n",
      "Iteration: 137 loss_NCC: 3.231e-01  loss_v: 1.021e-09 loss_J: 1.813e-05 loss_df: 1.311e-03 total_loss: 3.245e-01 time_cost: 1.250e+00 loss_MSE: 1.443e-03 loss_bdr: 1.239e-04 folding: 3.917e-05\n",
      "Iteration: 138 loss_NCC: 3.230e-01  loss_v: 1.025e-09 loss_J: 1.832e-05 loss_df: 1.305e-03 total_loss: 3.245e-01 time_cost: 1.251e+00 loss_MSE: 1.443e-03 loss_bdr: 1.230e-04 folding: 3.926e-05\n",
      "Iteration: 139 loss_NCC: 3.230e-01  loss_v: 1.028e-09 loss_J: 1.844e-05 loss_df: 1.299e-03 total_loss: 3.244e-01 time_cost: 1.256e+00 loss_MSE: 1.442e-03 loss_bdr: 1.221e-04 folding: 3.926e-05\n",
      "Iteration: 140 loss_NCC: 3.230e-01  loss_v: 1.032e-09 loss_J: 1.861e-05 loss_df: 1.293e-03 total_loss: 3.244e-01 time_cost: 1.249e+00 loss_MSE: 1.440e-03 loss_bdr: 1.211e-04 folding: 3.931e-05\n",
      "Iteration: 141 loss_NCC: 3.229e-01  loss_v: 1.033e-09 loss_J: 1.877e-05 loss_df: 1.287e-03 total_loss: 3.243e-01 time_cost: 1.251e+00 loss_MSE: 1.441e-03 loss_bdr: 1.202e-04 folding: 3.926e-05\n",
      "Iteration: 142 loss_NCC: 3.229e-01  loss_v: 1.037e-09 loss_J: 1.891e-05 loss_df: 1.282e-03 total_loss: 3.243e-01 time_cost: 1.245e+00 loss_MSE: 1.437e-03 loss_bdr: 1.193e-04 folding: 3.931e-05\n",
      "Iteration: 143 loss_NCC: 3.229e-01  loss_v: 1.039e-09 loss_J: 1.912e-05 loss_df: 1.277e-03 total_loss: 3.243e-01 time_cost: 1.249e+00 loss_MSE: 1.441e-03 loss_bdr: 1.184e-04 folding: 3.945e-05\n",
      "Iteration: 144 loss_NCC: 3.229e-01  loss_v: 1.043e-09 loss_J: 1.937e-05 loss_df: 1.271e-03 total_loss: 3.242e-01 time_cost: 1.249e+00 loss_MSE: 1.435e-03 loss_bdr: 1.176e-04 folding: 3.968e-05\n",
      "Iteration: 145 loss_NCC: 3.228e-01  loss_v: 1.046e-09 loss_J: 1.950e-05 loss_df: 1.266e-03 total_loss: 3.242e-01 time_cost: 1.244e+00 loss_MSE: 1.439e-03 loss_bdr: 1.167e-04 folding: 3.949e-05\n",
      "Iteration: 146 loss_NCC: 3.227e-01  loss_v: 1.049e-09 loss_J: 1.977e-05 loss_df: 1.261e-03 total_loss: 3.241e-01 time_cost: 1.247e+00 loss_MSE: 1.434e-03 loss_bdr: 1.159e-04 folding: 3.963e-05\n",
      "Iteration: 147 loss_NCC: 3.227e-01  loss_v: 1.053e-09 loss_J: 1.995e-05 loss_df: 1.257e-03 total_loss: 3.241e-01 time_cost: 1.246e+00 loss_MSE: 1.435e-03 loss_bdr: 1.152e-04 folding: 3.973e-05\n",
      "Iteration: 148 loss_NCC: 3.227e-01  loss_v: 1.054e-09 loss_J: 1.996e-05 loss_df: 1.251e-03 total_loss: 3.240e-01 time_cost: 1.250e+00 loss_MSE: 1.434e-03 loss_bdr: 1.143e-04 folding: 3.973e-05\n",
      "Iteration: 149 loss_NCC: 3.226e-01  loss_v: 1.058e-09 loss_J: 1.994e-05 loss_df: 1.247e-03 total_loss: 3.240e-01 time_cost: 1.247e+00 loss_MSE: 1.432e-03 loss_bdr: 1.136e-04 folding: 3.977e-05\n",
      "Iteration: 150 loss_NCC: 3.226e-01  loss_v: 1.066e-09 loss_J: 1.981e-05 loss_df: 1.244e-03 total_loss: 3.239e-01 time_cost: 1.252e+00 loss_MSE: 1.433e-03 loss_bdr: 1.132e-04 folding: 3.968e-05\n",
      "Iteration: 151 loss_NCC: 3.225e-01  loss_v: 1.055e-09 loss_J: 2.028e-05 loss_df: 1.235e-03 total_loss: 3.239e-01 time_cost: 1.252e+00 loss_MSE: 1.430e-03 loss_bdr: 1.117e-04 folding: 4.024e-05\n",
      "Iteration: 152 loss_NCC: 3.225e-01  loss_v: 1.109e-09 loss_J: 1.909e-05 loss_df: 1.248e-03 total_loss: 3.239e-01 time_cost: 1.248e+00 loss_MSE: 1.432e-03 loss_bdr: 1.139e-04 folding: 3.852e-05\n",
      "Iteration: 153 loss_NCC: 3.227e-01  loss_v: 1.002e-09 loss_J: 2.247e-05 loss_df: 1.208e-03 total_loss: 3.240e-01 time_cost: 1.244e+00 loss_MSE: 1.431e-03 loss_bdr: 1.076e-04 folding: 4.406e-05\n",
      "Iteration: 154 loss_NCC: 3.227e-01  loss_v: 1.018e-09 loss_J: 2.175e-05 loss_df: 1.206e-03 total_loss: 3.240e-01 time_cost: 1.250e+00 loss_MSE: 1.440e-03 loss_bdr: 1.075e-04 folding: 4.322e-05\n",
      "Iteration: 155 loss_NCC: 3.226e-01  loss_v: 1.029e-09 loss_J: 2.088e-05 loss_df: 1.206e-03 total_loss: 3.239e-01 time_cost: 1.269e+00 loss_MSE: 1.425e-03 loss_bdr: 1.077e-04 folding: 4.196e-05\n",
      "Iteration: 156 loss_NCC: 3.226e-01  loss_v: 1.047e-09 loss_J: 1.999e-05 loss_df: 1.209e-03 total_loss: 3.239e-01 time_cost: 1.274e+00 loss_MSE: 1.434e-03 loss_bdr: 1.079e-04 folding: 4.019e-05\n",
      "Iteration: 157 loss_NCC: 3.226e-01  loss_v: 1.138e-09 loss_J: 1.848e-05 loss_df: 1.236e-03 total_loss: 3.239e-01 time_cost: 1.248e+00 loss_MSE: 1.429e-03 loss_bdr: 1.120e-04 folding: 3.791e-05\n",
      "Iteration: 158 loss_NCC: 3.228e-01  loss_v: 9.859e-10 loss_J: 2.343e-05 loss_df: 1.179e-03 total_loss: 3.241e-01 time_cost: 1.247e+00 loss_MSE: 1.435e-03 loss_bdr: 1.034e-04 folding: 4.522e-05\n",
      "Iteration: 159 loss_NCC: 3.227e-01  loss_v: 9.873e-10 loss_J: 2.288e-05 loss_df: 1.174e-03 total_loss: 3.239e-01 time_cost: 1.250e+00 loss_MSE: 1.438e-03 loss_bdr: 1.027e-04 folding: 4.406e-05\n",
      "Iteration: 160 loss_NCC: 3.225e-01  loss_v: 9.942e-10 loss_J: 2.094e-05 loss_df: 1.173e-03 total_loss: 3.238e-01 time_cost: 1.252e+00 loss_MSE: 1.427e-03 loss_bdr: 1.029e-04 folding: 4.215e-05\n",
      "Iteration: 161 loss_NCC: 3.226e-01  loss_v: 1.002e-09 loss_J: 2.006e-05 loss_df: 1.174e-03 total_loss: 3.238e-01 time_cost: 1.249e+00 loss_MSE: 1.433e-03 loss_bdr: 1.028e-04 folding: 4.057e-05\n",
      "Iteration: 162 loss_NCC: 3.225e-01  loss_v: 1.049e-09 loss_J: 1.954e-05 loss_df: 1.184e-03 total_loss: 3.237e-01 time_cost: 1.251e+00 loss_MSE: 1.428e-03 loss_bdr: 1.042e-04 folding: 3.926e-05\n",
      "Iteration: 163 loss_NCC: 3.224e-01  loss_v: 1.070e-09 loss_J: 1.990e-05 loss_df: 1.186e-03 total_loss: 3.237e-01 time_cost: 1.251e+00 loss_MSE: 1.427e-03 loss_bdr: 1.046e-04 folding: 3.935e-05\n",
      "Iteration: 164 loss_NCC: 3.224e-01  loss_v: 1.063e-09 loss_J: 2.055e-05 loss_df: 1.181e-03 total_loss: 3.237e-01 time_cost: 1.248e+00 loss_MSE: 1.429e-03 loss_bdr: 1.036e-04 folding: 4.029e-05\n",
      "Iteration: 165 loss_NCC: 3.223e-01  loss_v: 1.064e-09 loss_J: 2.030e-05 loss_df: 1.177e-03 total_loss: 3.236e-01 time_cost: 1.253e+00 loss_MSE: 1.425e-03 loss_bdr: 1.030e-04 folding: 3.935e-05\n",
      "Iteration: 166 loss_NCC: 3.223e-01  loss_v: 1.055e-09 loss_J: 2.050e-05 loss_df: 1.170e-03 total_loss: 3.235e-01 time_cost: 1.254e+00 loss_MSE: 1.425e-03 loss_bdr: 1.018e-04 folding: 4.024e-05\n",
      "Iteration: 167 loss_NCC: 3.222e-01  loss_v: 1.040e-09 loss_J: 2.106e-05 loss_df: 1.162e-03 total_loss: 3.235e-01 time_cost: 1.251e+00 loss_MSE: 1.425e-03 loss_bdr: 1.004e-04 folding: 4.150e-05\n",
      "Iteration: 168 loss_NCC: 3.222e-01  loss_v: 1.047e-09 loss_J: 2.028e-05 loss_df: 1.160e-03 total_loss: 3.235e-01 time_cost: 1.251e+00 loss_MSE: 1.425e-03 loss_bdr: 1.001e-04 folding: 4.061e-05\n",
      "Iteration: 169 loss_NCC: 3.222e-01  loss_v: 1.054e-09 loss_J: 2.082e-05 loss_df: 1.159e-03 total_loss: 3.234e-01 time_cost: 1.248e+00 loss_MSE: 1.422e-03 loss_bdr: 1.001e-04 folding: 4.084e-05\n",
      "Iteration: 170 loss_NCC: 3.221e-01  loss_v: 1.055e-09 loss_J: 2.226e-05 loss_df: 1.157e-03 total_loss: 3.234e-01 time_cost: 1.250e+00 loss_MSE: 1.421e-03 loss_bdr: 9.955e-05 folding: 4.248e-05\n",
      "Iteration: 171 loss_NCC: 3.221e-01  loss_v: 1.064e-09 loss_J: 2.213e-05 loss_df: 1.156e-03 total_loss: 3.233e-01 time_cost: 1.254e+00 loss_MSE: 1.421e-03 loss_bdr: 9.909e-05 folding: 4.215e-05\n",
      "Iteration: 172 loss_NCC: 3.220e-01  loss_v: 1.066e-09 loss_J: 2.160e-05 loss_df: 1.154e-03 total_loss: 3.233e-01 time_cost: 1.250e+00 loss_MSE: 1.420e-03 loss_bdr: 9.858e-05 folding: 4.122e-05\n",
      "Iteration: 173 loss_NCC: 3.220e-01  loss_v: 1.064e-09 loss_J: 2.156e-05 loss_df: 1.150e-03 total_loss: 3.233e-01 time_cost: 1.250e+00 loss_MSE: 1.419e-03 loss_bdr: 9.792e-05 folding: 4.126e-05\n",
      "Iteration: 174 loss_NCC: 3.220e-01  loss_v: 1.067e-09 loss_J: 2.154e-05 loss_df: 1.147e-03 total_loss: 3.232e-01 time_cost: 1.256e+00 loss_MSE: 1.419e-03 loss_bdr: 9.746e-05 folding: 4.164e-05\n",
      "Iteration: 175 loss_NCC: 3.219e-01  loss_v: 1.058e-09 loss_J: 2.227e-05 loss_df: 1.142e-03 total_loss: 3.232e-01 time_cost: 1.248e+00 loss_MSE: 1.417e-03 loss_bdr: 9.663e-05 folding: 4.248e-05\n",
      "Iteration: 176 loss_NCC: 3.219e-01  loss_v: 1.057e-09 loss_J: 2.274e-05 loss_df: 1.138e-03 total_loss: 3.232e-01 time_cost: 1.252e+00 loss_MSE: 1.416e-03 loss_bdr: 9.598e-05 folding: 4.317e-05\n",
      "Iteration: 177 loss_NCC: 3.219e-01  loss_v: 1.062e-09 loss_J: 2.246e-05 loss_df: 1.137e-03 total_loss: 3.231e-01 time_cost: 1.247e+00 loss_MSE: 1.417e-03 loss_bdr: 9.573e-05 folding: 4.280e-05\n",
      "Iteration: 178 loss_NCC: 3.219e-01  loss_v: 1.055e-09 loss_J: 2.269e-05 loss_df: 1.132e-03 total_loss: 3.231e-01 time_cost: 1.247e+00 loss_MSE: 1.414e-03 loss_bdr: 9.490e-05 folding: 4.275e-05\n",
      "Iteration: 179 loss_NCC: 3.218e-01  loss_v: 1.087e-09 loss_J: 2.158e-05 loss_df: 1.142e-03 total_loss: 3.231e-01 time_cost: 1.246e+00 loss_MSE: 1.416e-03 loss_bdr: 9.585e-05 folding: 4.136e-05\n",
      "Iteration: 180 loss_NCC: 3.219e-01  loss_v: 1.040e-09 loss_J: 2.322e-05 loss_df: 1.121e-03 total_loss: 3.231e-01 time_cost: 1.249e+00 loss_MSE: 1.414e-03 loss_bdr: 9.315e-05 folding: 4.355e-05\n",
      "Iteration: 181 loss_NCC: 3.218e-01  loss_v: 1.051e-09 loss_J: 2.232e-05 loss_df: 1.122e-03 total_loss: 3.230e-01 time_cost: 1.251e+00 loss_MSE: 1.416e-03 loss_bdr: 9.323e-05 folding: 4.289e-05\n",
      "Iteration: 182 loss_NCC: 3.218e-01  loss_v: 1.036e-09 loss_J: 2.312e-05 loss_df: 1.114e-03 total_loss: 3.230e-01 time_cost: 1.249e+00 loss_MSE: 1.413e-03 loss_bdr: 9.195e-05 folding: 4.364e-05\n",
      "Iteration: 183 loss_NCC: 3.218e-01  loss_v: 1.043e-09 loss_J: 2.262e-05 loss_df: 1.115e-03 total_loss: 3.230e-01 time_cost: 1.251e+00 loss_MSE: 1.415e-03 loss_bdr: 9.207e-05 folding: 4.275e-05\n",
      "Iteration: 184 loss_NCC: 3.218e-01  loss_v: 1.063e-09 loss_J: 2.186e-05 loss_df: 1.119e-03 total_loss: 3.230e-01 time_cost: 1.254e+00 loss_MSE: 1.411e-03 loss_bdr: 9.253e-05 folding: 4.182e-05\n",
      "Iteration: 185 loss_NCC: 3.217e-01  loss_v: 1.052e-09 loss_J: 2.304e-05 loss_df: 1.113e-03 total_loss: 3.229e-01 time_cost: 1.246e+00 loss_MSE: 1.417e-03 loss_bdr: 9.152e-05 folding: 4.275e-05\n",
      "Iteration: 186 loss_NCC: 3.217e-01  loss_v: 1.077e-09 loss_J: 2.218e-05 loss_df: 1.120e-03 total_loss: 3.229e-01 time_cost: 1.249e+00 loss_MSE: 1.409e-03 loss_bdr: 9.257e-05 folding: 4.182e-05\n",
      "Iteration: 187 loss_NCC: 3.217e-01  loss_v: 1.022e-09 loss_J: 2.379e-05 loss_df: 1.097e-03 total_loss: 3.229e-01 time_cost: 1.249e+00 loss_MSE: 1.415e-03 loss_bdr: 8.930e-05 folding: 4.406e-05\n",
      "Iteration: 188 loss_NCC: 3.217e-01  loss_v: 1.031e-09 loss_J: 2.304e-05 loss_df: 1.097e-03 total_loss: 3.229e-01 time_cost: 1.245e+00 loss_MSE: 1.412e-03 loss_bdr: 8.935e-05 folding: 4.341e-05\n",
      "Iteration: 189 loss_NCC: 3.216e-01  loss_v: 1.040e-09 loss_J: 2.253e-05 loss_df: 1.099e-03 total_loss: 3.228e-01 time_cost: 1.244e+00 loss_MSE: 1.409e-03 loss_bdr: 8.957e-05 folding: 4.206e-05\n",
      "Iteration: 190 loss_NCC: 3.216e-01  loss_v: 1.060e-09 loss_J: 2.235e-05 loss_df: 1.106e-03 total_loss: 3.228e-01 time_cost: 1.245e+00 loss_MSE: 1.412e-03 loss_bdr: 9.018e-05 folding: 4.089e-05\n",
      "Iteration: 191 loss_NCC: 3.216e-01  loss_v: 1.063e-09 loss_J: 2.311e-05 loss_df: 1.104e-03 total_loss: 3.228e-01 time_cost: 1.248e+00 loss_MSE: 1.409e-03 loss_bdr: 8.986e-05 folding: 4.164e-05\n",
      "Iteration: 192 loss_NCC: 3.216e-01  loss_v: 1.039e-09 loss_J: 2.438e-05 loss_df: 1.092e-03 total_loss: 3.228e-01 time_cost: 1.244e+00 loss_MSE: 1.411e-03 loss_bdr: 8.820e-05 folding: 4.359e-05\n",
      "Iteration: 193 loss_NCC: 3.216e-01  loss_v: 1.058e-09 loss_J: 2.307e-05 loss_df: 1.099e-03 total_loss: 3.228e-01 time_cost: 1.249e+00 loss_MSE: 1.408e-03 loss_bdr: 8.888e-05 folding: 4.201e-05\n",
      "Iteration: 194 loss_NCC: 3.215e-01  loss_v: 1.039e-09 loss_J: 2.346e-05 loss_df: 1.089e-03 total_loss: 3.227e-01 time_cost: 1.252e+00 loss_MSE: 1.409e-03 loss_bdr: 8.748e-05 folding: 4.303e-05\n",
      "Iteration: 195 loss_NCC: 3.215e-01  loss_v: 1.076e-09 loss_J: 2.241e-05 loss_df: 1.102e-03 total_loss: 3.227e-01 time_cost: 1.254e+00 loss_MSE: 1.409e-03 loss_bdr: 8.904e-05 folding: 4.089e-05\n",
      "Iteration: 196 loss_NCC: 3.216e-01  loss_v: 1.017e-09 loss_J: 2.486e-05 loss_df: 1.076e-03 total_loss: 3.228e-01 time_cost: 1.249e+00 loss_MSE: 1.408e-03 loss_bdr: 8.560e-05 folding: 4.476e-05\n",
      "Iteration: 197 loss_NCC: 3.217e-01  loss_v: 1.119e-09 loss_J: 2.164e-05 loss_df: 1.118e-03 total_loss: 3.229e-01 time_cost: 1.249e+00 loss_MSE: 1.416e-03 loss_bdr: 9.056e-05 folding: 3.931e-05\n",
      "Iteration: 198 loss_NCC: 3.222e-01  loss_v: 9.812e-10 loss_J: 2.736e-05 loss_df: 1.058e-03 total_loss: 3.233e-01 time_cost: 1.246e+00 loss_MSE: 1.414e-03 loss_bdr: 8.317e-05 folding: 4.727e-05\n",
      "Iteration: 199 loss_NCC: 3.221e-01  loss_v: 9.854e-10 loss_J: 2.554e-05 loss_df: 1.055e-03 total_loss: 3.233e-01 time_cost: 1.254e+00 loss_MSE: 1.432e-03 loss_bdr: 8.270e-05 folding: 4.639e-05\n",
      "Iteration: 200 loss_NCC: 3.218e-01  loss_v: 9.895e-10 loss_J: 2.461e-05 loss_df: 1.055e-03 total_loss: 3.229e-01 time_cost: 1.244e+00 loss_MSE: 1.406e-03 loss_bdr: 8.291e-05 folding: 4.490e-05\n",
      "Iteration: 201 loss_NCC: 3.217e-01  loss_v: 9.935e-10 loss_J: 2.418e-05 loss_df: 1.059e-03 total_loss: 3.229e-01 time_cost: 1.250e+00 loss_MSE: 1.416e-03 loss_bdr: 8.299e-05 folding: 4.243e-05\n",
      "Iteration: 202 loss_NCC: 3.217e-01  loss_v: 1.045e-09 loss_J: 2.259e-05 loss_df: 1.075e-03 total_loss: 3.228e-01 time_cost: 1.246e+00 loss_MSE: 1.412e-03 loss_bdr: 8.529e-05 folding: 4.043e-05\n",
      "Iteration: 203 loss_NCC: 3.217e-01  loss_v: 1.015e-09 loss_J: 2.506e-05 loss_df: 1.060e-03 total_loss: 3.229e-01 time_cost: 1.250e+00 loss_MSE: 1.411e-03 loss_bdr: 8.335e-05 folding: 4.271e-05\n",
      "Iteration: 204 loss_NCC: 3.217e-01  loss_v: 1.008e-09 loss_J: 2.510e-05 loss_df: 1.056e-03 total_loss: 3.228e-01 time_cost: 1.246e+00 loss_MSE: 1.417e-03 loss_bdr: 8.266e-05 folding: 4.248e-05\n",
      "Iteration: 205 loss_NCC: 3.216e-01  loss_v: 1.011e-09 loss_J: 2.327e-05 loss_df: 1.054e-03 total_loss: 3.227e-01 time_cost: 1.246e+00 loss_MSE: 1.405e-03 loss_bdr: 8.279e-05 folding: 4.164e-05\n",
      "Iteration: 206 loss_NCC: 3.215e-01  loss_v: 1.001e-09 loss_J: 2.391e-05 loss_df: 1.051e-03 total_loss: 3.227e-01 time_cost: 1.245e+00 loss_MSE: 1.413e-03 loss_bdr: 8.196e-05 folding: 4.285e-05\n",
      "Iteration: 207 loss_NCC: 3.215e-01  loss_v: 1.016e-09 loss_J: 2.394e-05 loss_df: 1.054e-03 total_loss: 3.227e-01 time_cost: 1.247e+00 loss_MSE: 1.406e-03 loss_bdr: 8.232e-05 folding: 4.159e-05\n",
      "Iteration: 208 loss_NCC: 3.215e-01  loss_v: 1.020e-09 loss_J: 2.310e-05 loss_df: 1.055e-03 total_loss: 3.226e-01 time_cost: 1.246e+00 loss_MSE: 1.407e-03 loss_bdr: 8.246e-05 folding: 4.047e-05\n",
      "Iteration: 209 loss_NCC: 3.214e-01  loss_v: 1.024e-09 loss_J: 2.375e-05 loss_df: 1.057e-03 total_loss: 3.226e-01 time_cost: 1.247e+00 loss_MSE: 1.410e-03 loss_bdr: 8.227e-05 folding: 4.084e-05\n",
      "Iteration: 210 loss_NCC: 3.214e-01  loss_v: 1.035e-09 loss_J: 2.412e-05 loss_df: 1.058e-03 total_loss: 3.225e-01 time_cost: 1.252e+00 loss_MSE: 1.402e-03 loss_bdr: 8.258e-05 folding: 4.089e-05\n",
      "Iteration: 211 loss_NCC: 3.214e-01  loss_v: 1.028e-09 loss_J: 2.368e-05 loss_df: 1.056e-03 total_loss: 3.225e-01 time_cost: 1.250e+00 loss_MSE: 1.406e-03 loss_bdr: 8.187e-05 folding: 4.066e-05\n",
      "Iteration: 212 loss_NCC: 3.214e-01  loss_v: 1.034e-09 loss_J: 2.371e-05 loss_df: 1.055e-03 total_loss: 3.225e-01 time_cost: 1.249e+00 loss_MSE: 1.405e-03 loss_bdr: 8.168e-05 folding: 4.089e-05\n",
      "Iteration: 213 loss_NCC: 3.213e-01  loss_v: 1.033e-09 loss_J: 2.396e-05 loss_df: 1.054e-03 total_loss: 3.225e-01 time_cost: 1.250e+00 loss_MSE: 1.403e-03 loss_bdr: 8.164e-05 folding: 4.173e-05\n",
      "Iteration: 214 loss_NCC: 3.213e-01  loss_v: 1.027e-09 loss_J: 2.388e-05 loss_df: 1.051e-03 total_loss: 3.224e-01 time_cost: 1.249e+00 loss_MSE: 1.404e-03 loss_bdr: 8.088e-05 folding: 4.266e-05\n",
      "Iteration: 215 loss_NCC: 3.212e-01  loss_v: 1.030e-09 loss_J: 2.434e-05 loss_df: 1.050e-03 total_loss: 3.224e-01 time_cost: 1.249e+00 loss_MSE: 1.402e-03 loss_bdr: 8.062e-05 folding: 4.317e-05\n",
      "Iteration: 216 loss_NCC: 3.212e-01  loss_v: 1.028e-09 loss_J: 2.446e-05 loss_df: 1.048e-03 total_loss: 3.224e-01 time_cost: 1.249e+00 loss_MSE: 1.402e-03 loss_bdr: 8.044e-05 folding: 4.294e-05\n",
      "Iteration: 217 loss_NCC: 3.212e-01  loss_v: 1.029e-09 loss_J: 2.423e-05 loss_df: 1.047e-03 total_loss: 3.223e-01 time_cost: 1.245e+00 loss_MSE: 1.402e-03 loss_bdr: 8.006e-05 folding: 4.299e-05\n",
      "Iteration: 218 loss_NCC: 3.212e-01  loss_v: 1.032e-09 loss_J: 2.474e-05 loss_df: 1.047e-03 total_loss: 3.223e-01 time_cost: 1.252e+00 loss_MSE: 1.402e-03 loss_bdr: 7.983e-05 folding: 4.327e-05\n",
      "Iteration: 219 loss_NCC: 3.212e-01  loss_v: 1.029e-09 loss_J: 2.449e-05 loss_df: 1.045e-03 total_loss: 3.223e-01 time_cost: 1.271e+00 loss_MSE: 1.399e-03 loss_bdr: 7.958e-05 folding: 4.345e-05\n",
      "Iteration: 220 loss_NCC: 3.212e-01  loss_v: 1.030e-09 loss_J: 2.447e-05 loss_df: 1.044e-03 total_loss: 3.223e-01 time_cost: 1.264e+00 loss_MSE: 1.401e-03 loss_bdr: 7.934e-05 folding: 4.355e-05\n",
      "Iteration: 221 loss_NCC: 3.211e-01  loss_v: 1.032e-09 loss_J: 2.482e-05 loss_df: 1.043e-03 total_loss: 3.223e-01 time_cost: 1.266e+00 loss_MSE: 1.400e-03 loss_bdr: 7.902e-05 folding: 4.401e-05\n",
      "Iteration: 222 loss_NCC: 3.211e-01  loss_v: 1.029e-09 loss_J: 2.473e-05 loss_df: 1.042e-03 total_loss: 3.222e-01 time_cost: 1.264e+00 loss_MSE: 1.399e-03 loss_bdr: 7.873e-05 folding: 4.397e-05\n",
      "Iteration: 223 loss_NCC: 3.211e-01  loss_v: 1.029e-09 loss_J: 2.448e-05 loss_df: 1.040e-03 total_loss: 3.222e-01 time_cost: 1.252e+00 loss_MSE: 1.398e-03 loss_bdr: 7.845e-05 folding: 4.397e-05\n",
      "Iteration: 224 loss_NCC: 3.211e-01  loss_v: 1.031e-09 loss_J: 2.476e-05 loss_df: 1.039e-03 total_loss: 3.222e-01 time_cost: 1.249e+00 loss_MSE: 1.399e-03 loss_bdr: 7.824e-05 folding: 4.443e-05\n",
      "Iteration: 225 loss_NCC: 3.211e-01  loss_v: 1.029e-09 loss_J: 2.414e-05 loss_df: 1.038e-03 total_loss: 3.222e-01 time_cost: 1.253e+00 loss_MSE: 1.397e-03 loss_bdr: 7.801e-05 folding: 4.355e-05\n",
      "Iteration: 226 loss_NCC: 3.211e-01  loss_v: 1.030e-09 loss_J: 2.497e-05 loss_df: 1.037e-03 total_loss: 3.222e-01 time_cost: 1.250e+00 loss_MSE: 1.398e-03 loss_bdr: 7.764e-05 folding: 4.494e-05\n",
      "Iteration: 227 loss_NCC: 3.210e-01  loss_v: 1.030e-09 loss_J: 2.471e-05 loss_df: 1.036e-03 total_loss: 3.221e-01 time_cost: 1.258e+00 loss_MSE: 1.397e-03 loss_bdr: 7.752e-05 folding: 4.490e-05\n",
      "Iteration: 228 loss_NCC: 3.210e-01  loss_v: 1.029e-09 loss_J: 2.470e-05 loss_df: 1.034e-03 total_loss: 3.221e-01 time_cost: 1.252e+00 loss_MSE: 1.397e-03 loss_bdr: 7.715e-05 folding: 4.494e-05\n",
      "Iteration: 229 loss_NCC: 3.210e-01  loss_v: 1.030e-09 loss_J: 2.504e-05 loss_df: 1.034e-03 total_loss: 3.221e-01 time_cost: 1.249e+00 loss_MSE: 1.397e-03 loss_bdr: 7.699e-05 folding: 4.597e-05\n",
      "Iteration: 230 loss_NCC: 3.210e-01  loss_v: 1.030e-09 loss_J: 2.486e-05 loss_df: 1.032e-03 total_loss: 3.221e-01 time_cost: 1.251e+00 loss_MSE: 1.396e-03 loss_bdr: 7.669e-05 folding: 4.560e-05\n",
      "Iteration: 231 loss_NCC: 3.210e-01  loss_v: 1.027e-09 loss_J: 2.453e-05 loss_df: 1.031e-03 total_loss: 3.221e-01 time_cost: 1.253e+00 loss_MSE: 1.396e-03 loss_bdr: 7.627e-05 folding: 4.536e-05\n",
      "Iteration: 232 loss_NCC: 3.210e-01  loss_v: 1.031e-09 loss_J: 2.485e-05 loss_df: 1.030e-03 total_loss: 3.221e-01 time_cost: 1.248e+00 loss_MSE: 1.395e-03 loss_bdr: 7.621e-05 folding: 4.601e-05\n",
      "Iteration: 233 loss_NCC: 3.209e-01  loss_v: 1.029e-09 loss_J: 2.489e-05 loss_df: 1.029e-03 total_loss: 3.220e-01 time_cost: 1.247e+00 loss_MSE: 1.396e-03 loss_bdr: 7.591e-05 folding: 4.597e-05\n",
      "Iteration: 234 loss_NCC: 3.209e-01  loss_v: 1.033e-09 loss_J: 2.503e-05 loss_df: 1.030e-03 total_loss: 3.220e-01 time_cost: 1.250e+00 loss_MSE: 1.395e-03 loss_bdr: 7.591e-05 folding: 4.676e-05\n",
      "Iteration: 235 loss_NCC: 3.209e-01  loss_v: 1.029e-09 loss_J: 2.551e-05 loss_df: 1.027e-03 total_loss: 3.220e-01 time_cost: 1.255e+00 loss_MSE: 1.395e-03 loss_bdr: 7.539e-05 folding: 4.746e-05\n",
      "Iteration: 236 loss_NCC: 3.209e-01  loss_v: 1.032e-09 loss_J: 2.503e-05 loss_df: 1.027e-03 total_loss: 3.220e-01 time_cost: 1.248e+00 loss_MSE: 1.396e-03 loss_bdr: 7.535e-05 folding: 4.727e-05\n",
      "Iteration: 237 loss_NCC: 3.209e-01  loss_v: 1.022e-09 loss_J: 2.578e-05 loss_df: 1.022e-03 total_loss: 3.220e-01 time_cost: 1.255e+00 loss_MSE: 1.394e-03 loss_bdr: 7.466e-05 folding: 4.834e-05\n",
      "Iteration: 238 loss_NCC: 3.209e-01  loss_v: 1.039e-09 loss_J: 2.510e-05 loss_df: 1.029e-03 total_loss: 3.220e-01 time_cost: 1.251e+00 loss_MSE: 1.396e-03 loss_bdr: 7.531e-05 folding: 4.760e-05\n",
      "Iteration: 239 loss_NCC: 3.210e-01  loss_v: 1.012e-09 loss_J: 2.624e-05 loss_df: 1.016e-03 total_loss: 3.220e-01 time_cost: 1.250e+00 loss_MSE: 1.394e-03 loss_bdr: 7.362e-05 folding: 4.909e-05\n",
      "Iteration: 240 loss_NCC: 3.210e-01  loss_v: 1.049e-09 loss_J: 2.442e-05 loss_df: 1.032e-03 total_loss: 3.221e-01 time_cost: 1.250e+00 loss_MSE: 1.398e-03 loss_bdr: 7.539e-05 folding: 4.676e-05\n",
      "Iteration: 241 loss_NCC: 3.211e-01  loss_v: 9.924e-10 loss_J: 2.790e-05 loss_df: 1.004e-03 total_loss: 3.222e-01 time_cost: 1.245e+00 loss_MSE: 1.396e-03 loss_bdr: 7.202e-05 folding: 5.137e-05\n",
      "Iteration: 242 loss_NCC: 3.211e-01  loss_v: 1.005e-09 loss_J: 2.624e-05 loss_df: 1.006e-03 total_loss: 3.222e-01 time_cost: 1.251e+00 loss_MSE: 1.402e-03 loss_bdr: 7.246e-05 folding: 4.997e-05\n",
      "Iteration: 243 loss_NCC: 3.210e-01  loss_v: 9.877e-10 loss_J: 2.707e-05 loss_df: 9.989e-04 total_loss: 3.221e-01 time_cost: 1.249e+00 loss_MSE: 1.394e-03 loss_bdr: 7.153e-05 folding: 5.035e-05\n",
      "Iteration: 244 loss_NCC: 3.209e-01  loss_v: 9.958e-10 loss_J: 2.541e-05 loss_df: 1.002e-03 total_loss: 3.220e-01 time_cost: 1.245e+00 loss_MSE: 1.396e-03 loss_bdr: 7.183e-05 folding: 4.825e-05\n",
      "Iteration: 245 loss_NCC: 3.209e-01  loss_v: 1.008e-09 loss_J: 2.578e-05 loss_df: 1.006e-03 total_loss: 3.220e-01 time_cost: 1.248e+00 loss_MSE: 1.395e-03 loss_bdr: 7.225e-05 folding: 4.862e-05\n",
      "Iteration: 246 loss_NCC: 3.209e-01  loss_v: 1.018e-09 loss_J: 2.627e-05 loss_df: 1.012e-03 total_loss: 3.220e-01 time_cost: 1.246e+00 loss_MSE: 1.393e-03 loss_bdr: 7.275e-05 folding: 4.848e-05\n",
      "Iteration: 247 loss_NCC: 3.209e-01  loss_v: 1.030e-09 loss_J: 2.567e-05 loss_df: 1.015e-03 total_loss: 3.220e-01 time_cost: 1.247e+00 loss_MSE: 1.396e-03 loss_bdr: 7.321e-05 folding: 4.848e-05\n",
      "Iteration: 248 loss_NCC: 3.209e-01  loss_v: 9.961e-10 loss_J: 2.810e-05 loss_df: 9.991e-04 total_loss: 3.220e-01 time_cost: 1.252e+00 loss_MSE: 1.396e-03 loss_bdr: 7.105e-05 folding: 5.170e-05\n",
      "Iteration: 249 loss_NCC: 3.209e-01  loss_v: 1.000e-09 loss_J: 2.653e-05 loss_df: 9.972e-04 total_loss: 3.220e-01 time_cost: 1.246e+00 loss_MSE: 1.395e-03 loss_bdr: 7.100e-05 folding: 5.044e-05\n",
      "Iteration: 250 loss_NCC: 3.209e-01  loss_v: 9.912e-10 loss_J: 2.677e-05 loss_df: 9.951e-04 total_loss: 3.220e-01 time_cost: 1.249e+00 loss_MSE: 1.397e-03 loss_bdr: 7.043e-05 folding: 5.142e-05\n",
      "Iteration: 251 loss_NCC: 3.209e-01  loss_v: 1.002e-09 loss_J: 2.700e-05 loss_df: 9.976e-04 total_loss: 3.220e-01 time_cost: 1.250e+00 loss_MSE: 1.393e-03 loss_bdr: 7.080e-05 folding: 5.114e-05\n",
      "Iteration: 252 loss_NCC: 3.209e-01  loss_v: 1.009e-09 loss_J: 2.668e-05 loss_df: 1.001e-03 total_loss: 3.220e-01 time_cost: 1.248e+00 loss_MSE: 1.396e-03 loss_bdr: 7.098e-05 folding: 5.114e-05\n",
      "Iteration: 253 loss_NCC: 3.209e-01  loss_v: 1.011e-09 loss_J: 2.725e-05 loss_df: 1.002e-03 total_loss: 3.219e-01 time_cost: 1.253e+00 loss_MSE: 1.393e-03 loss_bdr: 7.100e-05 folding: 5.128e-05\n",
      "Iteration: 254 loss_NCC: 3.208e-01  loss_v: 1.020e-09 loss_J: 2.647e-05 loss_df: 1.006e-03 total_loss: 3.219e-01 time_cost: 1.252e+00 loss_MSE: 1.395e-03 loss_bdr: 7.132e-05 folding: 5.063e-05\n",
      "Iteration: 255 loss_NCC: 3.208e-01  loss_v: 9.993e-10 loss_J: 2.788e-05 loss_df: 9.939e-04 total_loss: 3.219e-01 time_cost: 1.253e+00 loss_MSE: 1.392e-03 loss_bdr: 6.994e-05 folding: 5.277e-05\n",
      "Iteration: 256 loss_NCC: 3.208e-01  loss_v: 9.952e-10 loss_J: 2.754e-05 loss_df: 9.911e-04 total_loss: 3.219e-01 time_cost: 1.254e+00 loss_MSE: 1.397e-03 loss_bdr: 6.944e-05 folding: 5.286e-05\n",
      "Iteration: 257 loss_NCC: 3.208e-01  loss_v: 9.907e-10 loss_J: 2.796e-05 loss_df: 9.880e-04 total_loss: 3.219e-01 time_cost: 1.250e+00 loss_MSE: 1.388e-03 loss_bdr: 6.920e-05 folding: 5.323e-05\n",
      "Iteration: 258 loss_NCC: 3.208e-01  loss_v: 9.945e-10 loss_J: 2.773e-05 loss_df: 9.898e-04 total_loss: 3.219e-01 time_cost: 1.250e+00 loss_MSE: 1.400e-03 loss_bdr: 6.909e-05 folding: 5.291e-05\n",
      "Iteration: 259 loss_NCC: 3.208e-01  loss_v: 1.001e-09 loss_J: 2.749e-05 loss_df: 9.916e-04 total_loss: 3.219e-01 time_cost: 1.250e+00 loss_MSE: 1.386e-03 loss_bdr: 6.942e-05 folding: 5.244e-05\n",
      "Iteration: 260 loss_NCC: 3.208e-01  loss_v: 1.004e-09 loss_J: 2.744e-05 loss_df: 9.934e-04 total_loss: 3.219e-01 time_cost: 1.247e+00 loss_MSE: 1.399e-03 loss_bdr: 6.930e-05 folding: 5.179e-05\n",
      "Iteration: 261 loss_NCC: 3.208e-01  loss_v: 1.007e-09 loss_J: 2.718e-05 loss_df: 9.939e-04 total_loss: 3.219e-01 time_cost: 1.248e+00 loss_MSE: 1.387e-03 loss_bdr: 6.936e-05 folding: 5.137e-05\n",
      "Iteration: 262 loss_NCC: 3.207e-01  loss_v: 1.008e-09 loss_J: 2.723e-05 loss_df: 9.937e-04 total_loss: 3.218e-01 time_cost: 1.249e+00 loss_MSE: 1.395e-03 loss_bdr: 6.916e-05 folding: 5.216e-05\n",
      "Iteration: 263 loss_NCC: 3.207e-01  loss_v: 1.004e-09 loss_J: 2.780e-05 loss_df: 9.920e-04 total_loss: 3.217e-01 time_cost: 1.246e+00 loss_MSE: 1.389e-03 loss_bdr: 6.888e-05 folding: 5.263e-05\n",
      "Iteration: 264 loss_NCC: 3.207e-01  loss_v: 1.004e-09 loss_J: 2.816e-05 loss_df: 9.903e-04 total_loss: 3.217e-01 time_cost: 1.248e+00 loss_MSE: 1.389e-03 loss_bdr: 6.856e-05 folding: 5.351e-05\n",
      "Iteration: 265 loss_NCC: 3.206e-01  loss_v: 1.001e-09 loss_J: 2.822e-05 loss_df: 9.895e-04 total_loss: 3.217e-01 time_cost: 1.246e+00 loss_MSE: 1.392e-03 loss_bdr: 6.837e-05 folding: 5.347e-05\n",
      "Iteration: 266 loss_NCC: 3.207e-01  loss_v: 1.004e-09 loss_J: 2.786e-05 loss_df: 9.890e-04 total_loss: 3.217e-01 time_cost: 1.251e+00 loss_MSE: 1.386e-03 loss_bdr: 6.820e-05 folding: 5.319e-05\n",
      "Iteration: 267 loss_NCC: 3.207e-01  loss_v: 1.002e-09 loss_J: 2.782e-05 loss_df: 9.891e-04 total_loss: 3.217e-01 time_cost: 1.248e+00 loss_MSE: 1.394e-03 loss_bdr: 6.798e-05 folding: 5.351e-05\n",
      "Iteration: 268 loss_NCC: 3.207e-01  loss_v: 1.007e-09 loss_J: 2.796e-05 loss_df: 9.895e-04 total_loss: 3.217e-01 time_cost: 1.254e+00 loss_MSE: 1.384e-03 loss_bdr: 6.795e-05 folding: 5.314e-05\n",
      "Iteration: 269 loss_NCC: 3.207e-01  loss_v: 1.003e-09 loss_J: 2.798e-05 loss_df: 9.888e-04 total_loss: 3.217e-01 time_cost: 1.252e+00 loss_MSE: 1.396e-03 loss_bdr: 6.766e-05 folding: 5.342e-05\n",
      "Iteration: 270 loss_NCC: 3.207e-01  loss_v: 1.005e-09 loss_J: 2.820e-05 loss_df: 9.872e-04 total_loss: 3.217e-01 time_cost: 1.253e+00 loss_MSE: 1.383e-03 loss_bdr: 6.756e-05 folding: 5.361e-05\n",
      "Iteration: 271 loss_NCC: 3.207e-01  loss_v: 1.003e-09 loss_J: 2.775e-05 loss_df: 9.865e-04 total_loss: 3.217e-01 time_cost: 1.256e+00 loss_MSE: 1.396e-03 loss_bdr: 6.719e-05 folding: 5.342e-05\n",
      "Iteration: 272 loss_NCC: 3.207e-01  loss_v: 1.002e-09 loss_J: 2.872e-05 loss_df: 9.850e-04 total_loss: 3.217e-01 time_cost: 1.252e+00 loss_MSE: 1.384e-03 loss_bdr: 6.709e-05 folding: 5.500e-05\n",
      "Iteration: 273 loss_NCC: 3.207e-01  loss_v: 1.003e-09 loss_J: 2.785e-05 loss_df: 9.842e-04 total_loss: 3.218e-01 time_cost: 1.249e+00 loss_MSE: 1.399e-03 loss_bdr: 6.670e-05 folding: 5.361e-05\n",
      "Iteration: 274 loss_NCC: 3.208e-01  loss_v: 9.995e-10 loss_J: 2.857e-05 loss_df: 9.829e-04 total_loss: 3.218e-01 time_cost: 1.258e+00 loss_MSE: 1.384e-03 loss_bdr: 6.661e-05 folding: 5.482e-05\n",
      "Iteration: 275 loss_NCC: 3.208e-01  loss_v: 1.003e-09 loss_J: 2.789e-05 loss_df: 9.827e-04 total_loss: 3.218e-01 time_cost: 1.249e+00 loss_MSE: 1.401e-03 loss_bdr: 6.638e-05 folding: 5.370e-05\n",
      "Iteration: 276 loss_NCC: 3.208e-01  loss_v: 9.952e-10 loss_J: 2.872e-05 loss_df: 9.799e-04 total_loss: 3.218e-01 time_cost: 1.249e+00 loss_MSE: 1.385e-03 loss_bdr: 6.619e-05 folding: 5.454e-05\n",
      "Iteration: 277 loss_NCC: 3.207e-01  loss_v: 1.006e-09 loss_J: 2.788e-05 loss_df: 9.826e-04 total_loss: 3.218e-01 time_cost: 1.247e+00 loss_MSE: 1.398e-03 loss_bdr: 6.618e-05 folding: 5.370e-05\n",
      "Iteration: 278 loss_NCC: 3.207e-01  loss_v: 9.842e-10 loss_J: 2.848e-05 loss_df: 9.729e-04 total_loss: 3.217e-01 time_cost: 1.248e+00 loss_MSE: 1.387e-03 loss_bdr: 6.536e-05 folding: 5.454e-05\n",
      "Iteration: 279 loss_NCC: 3.207e-01  loss_v: 9.928e-10 loss_J: 2.865e-05 loss_df: 9.736e-04 total_loss: 3.218e-01 time_cost: 1.251e+00 loss_MSE: 1.396e-03 loss_bdr: 6.513e-05 folding: 5.533e-05\n",
      "Iteration: 280 loss_NCC: 3.207e-01  loss_v: 9.830e-10 loss_J: 2.840e-05 loss_df: 9.702e-04 total_loss: 3.218e-01 time_cost: 1.249e+00 loss_MSE: 1.392e-03 loss_bdr: 6.498e-05 folding: 5.458e-05\n",
      "Iteration: 281 loss_NCC: 3.207e-01  loss_v: 9.926e-10 loss_J: 2.819e-05 loss_df: 9.734e-04 total_loss: 3.218e-01 time_cost: 1.246e+00 loss_MSE: 1.392e-03 loss_bdr: 6.500e-05 folding: 5.500e-05\n",
      "Iteration: 282 loss_NCC: 3.207e-01  loss_v: 9.921e-10 loss_J: 2.813e-05 loss_df: 9.730e-04 total_loss: 3.217e-01 time_cost: 1.249e+00 loss_MSE: 1.393e-03 loss_bdr: 6.515e-05 folding: 5.440e-05\n",
      "Iteration: 283 loss_NCC: 3.207e-01  loss_v: 9.979e-10 loss_J: 2.764e-05 loss_df: 9.763e-04 total_loss: 3.217e-01 time_cost: 1.254e+00 loss_MSE: 1.389e-03 loss_bdr: 6.528e-05 folding: 5.416e-05\n",
      "Iteration: 284 loss_NCC: 3.207e-01  loss_v: 9.857e-10 loss_J: 2.874e-05 loss_df: 9.679e-04 total_loss: 3.217e-01 time_cost: 1.247e+00 loss_MSE: 1.392e-03 loss_bdr: 6.447e-05 folding: 5.510e-05\n",
      "Iteration: 285 loss_NCC: 3.207e-01  loss_v: 9.833e-10 loss_J: 2.705e-05 loss_df: 9.667e-04 total_loss: 3.217e-01 time_cost: 1.249e+00 loss_MSE: 1.391e-03 loss_bdr: 6.436e-05 folding: 5.356e-05\n",
      "Iteration: 286 loss_NCC: 3.206e-01  loss_v: 9.806e-10 loss_J: 2.848e-05 loss_df: 9.644e-04 total_loss: 3.217e-01 time_cost: 1.249e+00 loss_MSE: 1.386e-03 loss_bdr: 6.394e-05 folding: 5.510e-05\n",
      "Iteration: 287 loss_NCC: 3.206e-01  loss_v: 9.881e-10 loss_J: 2.679e-05 loss_df: 9.676e-04 total_loss: 3.217e-01 time_cost: 1.249e+00 loss_MSE: 1.394e-03 loss_bdr: 6.428e-05 folding: 5.286e-05\n",
      "Iteration: 288 loss_NCC: 3.206e-01  loss_v: 9.877e-10 loss_J: 2.784e-05 loss_df: 9.680e-04 total_loss: 3.216e-01 time_cost: 1.247e+00 loss_MSE: 1.383e-03 loss_bdr: 6.421e-05 folding: 5.412e-05\n",
      "Iteration: 289 loss_NCC: 3.206e-01  loss_v: 9.956e-10 loss_J: 2.755e-05 loss_df: 9.706e-04 total_loss: 3.216e-01 time_cost: 1.247e+00 loss_MSE: 1.394e-03 loss_bdr: 6.431e-05 folding: 5.356e-05\n",
      "Iteration: 290 loss_NCC: 3.206e-01  loss_v: 9.944e-10 loss_J: 2.763e-05 loss_df: 9.704e-04 total_loss: 3.216e-01 time_cost: 1.249e+00 loss_MSE: 1.382e-03 loss_bdr: 6.436e-05 folding: 5.421e-05\n",
      "Iteration: 291 loss_NCC: 3.205e-01  loss_v: 9.918e-10 loss_J: 2.834e-05 loss_df: 9.691e-04 total_loss: 3.216e-01 time_cost: 1.248e+00 loss_MSE: 1.392e-03 loss_bdr: 6.396e-05 folding: 5.519e-05\n",
      "Iteration: 292 loss_NCC: 3.205e-01  loss_v: 9.935e-10 loss_J: 2.717e-05 loss_df: 9.690e-04 total_loss: 3.216e-01 time_cost: 1.251e+00 loss_MSE: 1.384e-03 loss_bdr: 6.395e-05 folding: 5.393e-05\n",
      "Iteration: 293 loss_NCC: 3.205e-01  loss_v: 9.866e-10 loss_J: 2.854e-05 loss_df: 9.666e-04 total_loss: 3.215e-01 time_cost: 1.255e+00 loss_MSE: 1.390e-03 loss_bdr: 6.343e-05 folding: 5.552e-05\n",
      "Iteration: 294 loss_NCC: 3.205e-01  loss_v: 9.912e-10 loss_J: 2.739e-05 loss_df: 9.670e-04 total_loss: 3.215e-01 time_cost: 1.253e+00 loss_MSE: 1.383e-03 loss_bdr: 6.357e-05 folding: 5.435e-05\n",
      "Iteration: 295 loss_NCC: 3.205e-01  loss_v: 9.889e-10 loss_J: 2.749e-05 loss_df: 9.674e-04 total_loss: 3.215e-01 time_cost: 1.251e+00 loss_MSE: 1.389e-03 loss_bdr: 6.328e-05 folding: 5.426e-05\n",
      "Iteration: 296 loss_NCC: 3.205e-01  loss_v: 9.886e-10 loss_J: 2.826e-05 loss_df: 9.646e-04 total_loss: 3.215e-01 time_cost: 1.248e+00 loss_MSE: 1.383e-03 loss_bdr: 6.303e-05 folding: 5.566e-05\n",
      "Iteration: 297 loss_NCC: 3.205e-01  loss_v: 9.902e-10 loss_J: 2.766e-05 loss_df: 9.674e-04 total_loss: 3.215e-01 time_cost: 1.248e+00 loss_MSE: 1.390e-03 loss_bdr: 6.313e-05 folding: 5.514e-05\n",
      "Iteration: 298 loss_NCC: 3.205e-01  loss_v: 9.791e-10 loss_J: 2.915e-05 loss_df: 9.577e-04 total_loss: 3.216e-01 time_cost: 1.251e+00 loss_MSE: 1.383e-03 loss_bdr: 6.223e-05 folding: 5.743e-05\n",
      "Iteration: 299 loss_NCC: 3.205e-01  loss_v: 9.761e-10 loss_J: 2.787e-05 loss_df: 9.580e-04 total_loss: 3.216e-01 time_cost: 1.254e+00 loss_MSE: 1.392e-03 loss_bdr: 6.218e-05 folding: 5.635e-05\n",
      "Iteration: 300 loss_NCC: 3.205e-01  loss_v: 9.775e-10 loss_J: 2.901e-05 loss_df: 9.559e-04 total_loss: 3.215e-01 time_cost: 1.249e+00 loss_MSE: 1.382e-03 loss_bdr: 6.208e-05 folding: 5.715e-05\n"
     ]
    }
   ],
   "source": [
    "#训练部分\n",
    "for i in range(epoches):\n",
    "        #开始计时\n",
    "        s_t = time.time()\n",
    "\n",
    "        all_phi = odeint(func = Network, y0 = grid, t=torch.tensor(train_times).to(device),method=\"rk4\",rtol=1e-3,atol=1e-5).to(device)\n",
    "      \n",
    "        #速度场（相邻两个时间点的状态相减，具体理解可参考euler法的定义）\n",
    "        all_v = all_phi[1:] - all_phi[:-1]\n",
    "        all_phi = (all_phi + 1.) / 2. * scale_factor  # [-1, 1] -> voxel spacing  恢复到标准的坐标系\n",
    "        grid_voxel = (grid + 1.) / 2. * scale_factor  # [-1, 1] -> voxel spacing\n",
    "        #记录各种loss\n",
    "        total_loss = 0.0\n",
    "        epoch_loss = []\n",
    "        epoch_loss_NCC = []\n",
    "        epoch_loss_MSE = []\n",
    "        epoch_loss_v = []\n",
    "        epoch_loss_J = []\n",
    "        epoch_folding = []\n",
    "        epoch_loss_df = []\n",
    "        epoch_loss_bdr = []\n",
    "        \n",
    "        seq_length = 7\n",
    "\n",
    "        #对每一个时间点的预测进行loss计算\n",
    "        for n in range(1,seq_length):\n",
    "            phi = all_phi[n]\n",
    "            df = phi - grid_voxel  # with grid -> without grid（此处的df是offset）\n",
    "            warped_moving, df_with_grid = ST(train_List[0], df, return_phi=True)\n",
    "            # similarity loss（NCC）\n",
    "            loss_sim = loss_NCC(warped_moving, train_List[n])\n",
    "            epoch_loss_NCC.append(loss_sim.clone().detach().cpu())\n",
    "            \n",
    "            #loss_ncc = loss_NCC(warped_moving,train_List[n])\n",
    "            #epoch_loss_NCC.append(loss_ncc.clone().detach().cpu())\n",
    "            \n",
    "            loss_mse = MSE(warped_moving,train_List[n])\n",
    "            epoch_loss_MSE.append(loss_mse.clone().detach().cpu())\n",
    "\n",
    "            warped_moving = warped_moving.squeeze(0).squeeze(0)\n",
    "            # V magnitude loss\n",
    "            loss_v = 0.00005 * magnitude_loss(all_v)\n",
    "            epoch_loss_v.append(loss_v.clone().detach().cpu())\n",
    "            # neg Jacobian loss\n",
    "            loss_J = 0.000001 * neg_Jdet_loss1(df_with_grid)\n",
    "            epoch_loss_J.append(loss_J.clone().detach().cpu())\n",
    "            \n",
    "            #folding\n",
    "            folding = calculate_folding(df,device)\n",
    "            epoch_folding.append(folding)\n",
    "            \n",
    "            \n",
    "            # phi dphi/dx loss\n",
    "            loss_df = 0.05 * smoothloss_loss(df)\n",
    "            epoch_loss_df.append(loss_df.clone().detach().cpu())\n",
    "            #bdr loss\n",
    "            loss_bdr = 0.0001*boundary_loss(df)\n",
    "            epoch_loss_bdr.append(loss_bdr.clone().detach().cpu())\n",
    "            #各项loss求和\n",
    "            loss = loss_sim + loss_df + loss_bdr\n",
    "            #+ loss_v + loss_J + loss_df\n",
    "            epoch_loss.append(loss.clone().detach().cpu())\n",
    "            #各个时间点的loss求和\n",
    "            total_loss = total_loss + loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = total_loss/(seq_length-1)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #结束计时\n",
    "        e_t = time.time()\n",
    "\n",
    "        print(\"Iteration: {0} loss_NCC: {1:.3e}  loss_v: {2:.3e} loss_J: {3:.3e} loss_df: {4:.3e} total_loss: {5:.3e} time_cost: {6:.3e} loss_MSE: {7:.3e} loss_bdr: {8:.3e} folding: {9:.3e}\"\n",
    "              .format(i + 1, \n",
    "                      np.mean(epoch_loss_NCC),\n",
    "                      np.mean(epoch_loss_v),\n",
    "                      np.mean(epoch_loss_J),\n",
    "                      np.mean(epoch_loss_df),\n",
    "                      total_loss.item(),\n",
    "                      e_t-s_t,\n",
    "                      np.mean(epoch_loss_MSE),\n",
    "                      np.mean(epoch_loss_bdr),\n",
    "                      np.mean(epoch_folding)\n",
    "                      )\n",
    "                      )\n",
    "        #保存每个epoch的记录\n",
    "        epoch_record = {\"Iteration\":i+1,\n",
    "                        \"loss_NCC\":np.mean(epoch_loss_NCC),\n",
    "                        \"loss_v\":np.mean(epoch_loss_v),\n",
    "                        \"loss_J\":np.mean(epoch_loss_J),\n",
    "                        \"folding\":np.mean(epoch_folding),\n",
    "                        \"loss_df\":np.mean(epoch_loss_df),\n",
    "                        \"loss_bdr\":np.mean(epoch_loss_bdr),\n",
    "                        \"total_loss\":total_loss.item(),\n",
    "                        \"time_cost\":e_t-s_t}\n",
    "        total_record.append(epoch_record)\n",
    "        #写入日志\n",
    "        #log_writer.add_scalar(tag=\"loss_NCC\",scalar_value=epoch_record[\"loss_NCC\"],global_step=i+1)\n",
    "        #log_writer.add_scalar(tag=\"loss_v\",scalar_value=epoch_record[\"loss_v\"],global_step=i+1)\n",
    "        #log_writer.add_scalar(tag=\"loss_J\",scalar_value=epoch_record[\"loss_J\"],global_step=i+1)\n",
    "        #log_writer.add_scalar(tag=\"folding\",scalar_value=epoch_record[\"folding\"],global_step=i+1)\n",
    "        #log_writer.add_scalar(tag=\"loss_df\",scalar_value=epoch_record[\"loss_df\"],global_step=i+1)\n",
    "        #log_writer.add_scalar(tag=\"loss_bdr\",scalar_value=epoch_record[\"loss_df\"],global_step=i+1)\n",
    "        #log_writer.add_scalar(tag=\"total_loss\",scalar_value=epoch_record[\"total_loss\"],global_step=i+1)\n",
    "        #log_writer.add_scalar(tag=\"time_cost\",scalar_value=epoch_record[\"time_cost\"],global_step=i+1) \n",
    "        \n",
    "        #每50个epoch保存一次模型\n",
    "        if (i+1)%50 == 0:\n",
    "            torch.save(Network.state_dict(),os.path.join(savePath,\"epoch-%d.pkl\"%(i+1)))\n",
    "        #log_writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
